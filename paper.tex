\documentclass[10pt, conference, compsocconf]{IEEEtran}

% packages
\usepackage{algorithm}
\usepackage{algorithmicx} % Doc is at http://tug.ctan.org/macros/latex/contrib/algorithmicx/algorithmicx.pdf
\usepackage{algpseudocode}
\usepackage{amsfonts} % for R symbol (the set of real numbers)
\usepackage{color}
\usepackage{colortbl} % for \rowcolor
\usepackage[pdftex]{graphicx}
\usepackage{graphicx}
\usepackage[bookmarks=false]{hyperref}
\hypersetup{colorlinks=true,linkcolor=black,citecolor=black,filecolor=black,urlcolor=blue}
\usepackage{mathtools}
\usepackage{multirow}
\usepackage{stmaryrd} % for llbracket and rrbracket
\usepackage{subcaption}
\usepackage{nicefrac}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{stmaryrd}
\usepackage{amsthm}
\DeclarePairedDelimiter{\ceil}{\lceil}{\rceil}
\DeclarePairedDelimiter{\floor}{\lfloor}{\rfloor}

% new commands
\newcommand{\todo}[1]{\marginpar{\parbox{18mm}{\flushleft\tiny\color{red}\textbf{TODO}:
      #1}}}
\newcommand{\note}[1]{
  \color{blue}\emph{[Note: #1]}
  \color{black}
}
\newcommand{\norm}[1]{\left\lVert#1\right\rVert}
\newtheorem*{theorem}{Theorem}
\definecolor{headcolor}{gray}{0.9}


\begin{document}

\title{A multi-dimensional extension of the Lightweight Temporal Compression method}

\author{Bo Li, Tristan Glatard\\
  Department of Computer Science and Software Engineering\\ Concordia University, Montreal, Quebec, Canada\\
  {first.last}@concordia.ca \vspace*{-0.5cm}}

\maketitle

\begin{abstract}
Lightweight Temporal Compression
(LTC~\cite{schoellhammer2004lightweight}) is among the stream
compression method that provides the highest compression rate for the
lowest resource (CPU, RAM) consumption. As such, it makes it a very
good candidate for the compression of data streams acquired in
embedded, low-power systems such as connected
objects, systems on module (SoMs), etc The type of data acquired on
such systems, however, is often multi-dimensional. For instance
accelerometers and gyroscopes usually measure variables along 3
directions, these variables being distinct but related through the
actual movement of the sensor. In this paper, we investigate the
extension of LTC to higher dimensions. First, we provide a formulation of the algorithm
in an arbitrary vectorial space of dimension $n$. Furthermore, we
implement the algorithm for the infinity and Euclidean norms, in spaces of
dimension 2D+t and 3D+t. Finally, we compare the multi-dimensional LTC
to the 1-dimensional version, on gyroscopic data acquired on a SoM
device.
\end{abstract}

\section{Introduction}

Section~\ref{sec:ltc} provides some background on the LTC algorithm.
Section~\ref{sec:extension} presents our formulation in dimension $n$.

% Some related works from Bo's report


% transmitted, received points

% contributions: formalization of LTC algorithm, extension to n d, extension to any norm, open-source implementation.

\section{Lightweight Temporal Compression}
\label{sec:ltc}

LTC approximates the data stream
by a piece-wise linear function of time, ensuring that the error at
every time-point is bounded.

\subsection{Notations}

The algorithm \emph{receives} a stream of \emph{data points} $x_i$
at times $t_i$ ($i \in \mathbb{N}$), and it \emph{transmits} a stream of data points $\xi_i$
at times $\tau_i$ ($i \in \mathbb{N}$). To simplify the notations, we assume that:
\begin{equation*}
\forall k \in \mathbb{N}, \  \exists ! i \in \mathbb{N} \  \tau_k = t_i
\end{equation*}
That is, transmission times coincide with reception times.
We define the \emph{centered received points} as follows:
\begin{equation*}
\forall k \in \mathbb{N}\ , \forall j \in \mathbb{N^*},\ (u^k_j, y^k_j) = (t_{i+j}, x_{i+j}), \quad \mathrm{where\ }i\mathrm{\ is\ s.t.}\ t_i = \tau_k.
\end{equation*}
and:
\begin{equation*}
\forall k \in \mathbb{N},\  (u^k_0, y^k_0) = (\tau_k, \xi_k).
\end{equation*}
This definition is such that $y^k_j$ is the $j^{th}$ data point received
after the $k^{th}$ transmission.
Figure~\ref{fig:ltc} illustrates the notations and algorithm.
\begin{figure}
\includegraphics[width=\columnwidth]{ltc.pdf}
\caption{Illustration of the LTC algorithm and notations used. Red dots indicate transmitted points. Dashed lines
represent the high line and the low line at times when a point is transmitted.}
\label{fig:ltc}
\end{figure}
\subsection{LTC algorithm}

The LTC algorithm maintains two lines, the \emph{high line}, and the
\emph{low line} defined by (1) the latest transmitted point and (2) the
\emph{high point} (high line) and the \emph{low point} (low line). When
a point ($t_i$, $x_i$) is received, the high line is updated as
follows: if $x_i+\epsilon$ is below the high line then the high line is
updated to the line defined by the last transmitted point and ($t_i$,
$x_i+\epsilon$); otherwise, the high line is not updated. Likewise, the low line
is updated from $x_i-\epsilon$. Therefore, any line located between the
high line and the low line approximates the data points received since
the last transmitted point with an error bounded by $\epsilon$.

Using these notations, the original LTC algorithm can
be written as in Algorithm~\ref{algo:ltc}. For readability, we assume
that access to data points is blocking, i.e., the program will wait
until the points are available. We also assume that the content of
variable \texttt{tr} is transmitted after each assignment of this
variable. Function \texttt{line}, omitted for brevity, returns the
ordinate at abscissa $x$ (1st argument) of the line defined by the points
in its 2nd and 3rd arguments.

\begin{algorithm}
\begin{algorithmic}[1]
\State tr = $(u^0_0, y^0_0)$ \Comment{last transmitted point}
\State k = 0 ; j = 1
\State (lp, hp) = ($y^1_1 - \epsilon$, $y^1_1 + \epsilon$) \Comment{low and high points}

\While{True} \Comment{Process received points as they come}
    \State j += 1
    \State new\_lp = max($y^k_j-\epsilon$, line($u^k_j$, tr, ($u^k_{j-1}$, lp)))
    \State new\_hp = max($y^k_j+\epsilon$, line($u^k_j$, tr, ($u^k_{j-1}$, hp)))
    \If{new\_lp $<$ new\_hp} \Comment{Keep compressing}
        \State (lp = new\_lp, new\_hp)
        \State \textbf{continue}
    \EndIf
    \State tr = $(u^k_j, (lp+hp)/2)$
    \Comment{Transmit point}
    \State (lp, hp) = ($y^k_j-\epsilon$, $y^k_j+\epsilon$)
    \State k += 1
    \State j = 0
\EndWhile
\end{algorithmic}
\caption{Original LTC algorithm, adapted from~\cite{schoellhammer2004lightweight}.}
\label{algo:ltc}
\end{algorithm}


 %~ is one of the linear estimation
%~ method to compress data. It is The first-order interpolator with two
%~ degrees of freedom (FOI-2DF) which is mentioned in
%~ ~\cite{jalaleddine1990ecg}.  Most of linear estimation method need a
%~ predefined parameter which is the error margin (let's call $\epsilon$)
%~ in order to guarantee the difference between estimation and original
%~ data within a scope.

%~ \todo{rewrite LTC algorithm, and there is a DP version of LTC maybe should also be written}

\section{Extension to dimension $n$}
\label{sec:extension}
In this section we provide a norm-independent formulation of LTC in
dimension $n$. By $n$ we refer to the dimension of the data points
$x_i$. To handle time, LTC actually operates in dimension
$n+1$.

\subsection{Preliminary comments}

We note that the formulation of LTC in~\cite{schoellhammer2004lightweight} relies on
the intersection of \emph{convex cones} in dimension $n+1$. For $n=1$, it
corresponds to the intersection of triangles, which can efficiently be
computed by maintaining boundary lines, as detailed previously. In higher dimension, however, cone intersections are not so
straighforward, due to the fact that the intersection between cones
may not be a cone.

To address this issue, we formulate LTC as an intersection test between
\emph{balls} of dimension $n$, that is, segments for $n=1$, disks for
$n=2$, etc. Balls are defined from the \emph{norm} used in
the vector space of data points. For $n=1$, the choice of the norm does
not really matter, as all p-norms and the infinity norm are identical.
In dimension $n$, however, norm selection will be critical.

\subsection{Algebraic formulation of LTC}

\subsubsection{Definitions}

Let $(u_0^k, y_0^k) \in \mathbb{R}^{n+1}$ be the latest transmitted point. For convenience, all the subsequent points will be
expressed in the orthogonal space with origin $(\tau^k, \xi^k)$. We denote by $(v_j, z_j)_{j \in \llbracket 0, m \rrbracket}$ such points:
\begin{equation*}
\forall j \leq m,\  (v_j, z_j) = (u_j^k - \tau^k, y_j^k - \xi^k)
\end{equation*}
We define $\mathcal{B}_j$ as the ball of $\mathbb{R}^n$ of center $\frac{1}{j}z_j$ and of radius
$\frac{\epsilon}{j}$:
\begin{equation*}
\mathcal{B}_j = \left\{ z \in \mathbb{R}^n,\  \norm{z-\frac{v_1}{v_j}z_j} \leq \frac{v_1}{v_j}\epsilon \right\}
\end{equation*}

\subsection{LTC property}

We formulate the \emph{LTC property} as follows:
\begin{equation*}
\exists z \in \mathbb{R}^n, \ \forall j \in \llbracket 1, m \rrbracket, \norm{\frac{v_j}{v_1}z-z_j} \leq \epsilon.
\end{equation*}
The LTC algorithm ensures that the LTC property is
verified between each transmission. Indeed, all the data points
$z$ such that $(v_1, z)$ is between the high line and the low line
verify the property. Line 9 in Algorithm~\ref{algo:ltc} guarantees that
such a point exists.
The LTC property can be written as follows:
\begin{equation*}
\exists z \in \mathbb{R}^n, \ \forall j \in \llbracket 1, m \rrbracket, \norm{z-\frac{v_1}{v_j}z_j} \leq \frac{v_1}{v_j}\epsilon
\end{equation*}
that is:
\begin{equation}
\bigcap_{j=1}^m \mathcal{B}_j \neq \O
\label{eq:ltc-property}
\end{equation}
Note that $(\mathcal{B}_j)_{j \in \llbracket 1, m \rrbracket}$ is a sequence
of balls of strictly decreasing radius, since $v_j > v_1$.
\todo{Add a figure to illustrate this}

\subsection{Algorithm}

The LTC algorithm generalized to dimension $n$ test, for each data point,
that the LTC property in Equation~\ref{eq:ltc-property} is verified. It is written in
Algorithm~\ref{algo:general-ltc}.
\begin{algorithm}
\begin{algorithmic}[1]
\State tr = ($\tau, \xi$) = ($u^0_0, y^0_0$)
\State k = 0 ; j = 0
\While{True}
    \State j += 1
    \State ($v_j, z_j$) = ($u_j^k - \tau, y_j^k - \xi$)
    \If{$\bigcap_{l=1}^j{\mathcal{B}_l} \neq \O$}
        \State \textbf{continue}
    \EndIf
    \State Pick $z$ in $\bigcap_{l=1}^{j-1}{\mathcal{B}_j}$
    \State tr = ($\tau$, $\xi$) = ($u^k_{j-1}, z$)
    \State k += 1
    \State j = 0
\EndWhile
\end{algorithmic}
\caption{Generalized LTC}
\label{algo:general-ltc}
\end{algorithm}

\todo{Check algorithms (reimplement)}

\subsection{Ball intersections}

Although Algorithm~\ref{algo:general-ltc} looks simple, one should not
overlook the fact that there is no good general algorithm to test
whether a set of balls intersect. The best general algorithm we could find
so far relies on Helly's theorem which is formulated as follows~\cite{helly1923mengen}:
\begin{theorem}
Let $\left\{ X_i \right\}_{i \in \llbracket 1, m \rrbracket}$ be a collection of convex subsets of $\mathbb{R}^n$. If the intersection of every $n+1$
subsets is non-empty, then the whole collection has an non-empty intersection.
\end{theorem}
This theorem leads to an algorithm of complexity ${m \choose n+1}$ which is
not usable in resource-constrained environment.

The only feasible algorithm that we found is norm-specific. It
maintains a representation of the intersection
$\bigcap_{j=1}^{m}{\mathcal{B}_m}$ which is updated at every iteration.
The intersection tests can then be done in constant time, however,
updating the representation of the intersection may be more costly
depending on the norm used. For the infinity norm, the representation
is a rectangular cuboid which is straightforward to update by
intersection with an n-ball, since the n-ball is a rectangular cuboid too.
For the Euclidean norm, the representation is a volume with no specific property,
which is more costly to maintain.

\subsection{Effect of the norm}

As mentioned before, norm selection in $\mathbb{R}^n$ has a critical
impact on the compression error and ratio. To appreciate this effect,
let us consider the comparison between the infinity norm and the
Euclidean norm in dimension 2. By comparing the unit disk to the unit
square, we obtain that the compression ratio of a random stream would
be $\frac{4}{\pi}$ times larger with the infinity norm than with the
Euclidean norm. Conversely, a compression error bounded by $\epsilon$
with the infinity norm corresponds to a compression error of
$\frac{\epsilon}{\sqrt{n}}$ with the Euclidean norm. Unsurprisingly, the
infinity norm is more 'tolerant' than the Euclidean norm.

\todo{Perhaps add a figure}

It should also be noted that using the infinity norm boils down to the
use of the 1D LTC algorithm independently in each dimension: a data
point is transmitted as soon as the linear approximation doesn't hold
in any of the coordinates. \todo{This is unclear}


\section{Implementation}
Our code is available at ... (make a clean release!)

As reference above, the norm selection is importance. In this section,
we show the implementation of infinity norm and Euclidean norm.

%~ In practice, It's needed to transmit raw data which has multi-parameters
%~ to clients e.g. Accelerometer, Euler Angle and Magnetometer etc.
%~ Most of previous paper just force on single dimension sensor
%~ data Compression. So we purpose a method to compress multi-dimensional
%~ sensor data named LTC-D. It is used as linear estimation method.
%~ Same as usual estimation compression method,  distance(different)
%~ with compressed data from LTC-D and original data would not exceed
%~ the predefined error range. We need to find line to represent
%~ original data, it means the lines pass through datas' error range.
%~ But in stream data, time seires data, it is difficult, because
%~ all data do not show in same time plane. So this method will
%~ mapping all data into one time plane. Then for each mapped error
%~ range of data, check if there is intersection among them. If so means
%~ there is a line could represent original data, and vice versa.
\subsection{Multidimensional LTC with Manhattan distance}
In section 2, we can know that the repersentation of intersection
is a rectangular cuboid. So only one bounding box with n-dimensional
is needed to represent the intersection. Assume a bounding box
$\mathcal{BOX}_{m-1} = \bigcap_{j=1}^{m-1}{\mathcal{B}_{m-1}}$ which
has Upper bound $U_i$ and lower Bound $L_i$ for each dimension
$i$. So the fomular (1) in $6_{th}$ line of Algorithm 2 would
be changed to $\mathcal{BOX}_{m-1} \bigcap \mathcal{B}_{m} \neq \O $. The Bounding box
would be updated when this formular return true.
Otherwise return false.
\begin{enumerate}
    \item Initialization: Let origin $(u^k_0, y^k_0) = (\tau_k, \xi_k)$.
    \item For each data $(u_m^k, y_m^k)$ with diminsion $n$,
            defines $(v_m, z_m) = (u_m^k - \tau^k, y_m^k - \xi^k)$ where $z_m =
            \{ x_{mi} \mid 1\leqslant{i}\leqslant{n}\}$.
    \item The $\mathcal{B}_{m}$ should be a ractangular cube with center
            $\frac{v_1}{v_m}z_m$ and weight $\frac{v_1}{v_m}\epsilon$.
    \item For each dimension $i$, box upper bound $U_i < \frac{v_1}{v_m}x_{mi}+
            \frac{v_1}{v_m}\epsilon$ OR lower bound $L_i > \frac{v_1}{v_m}x_{mi}-\frac{v_1}{v_m}\epsilon$, return False.
    \item Otherwise, the intersection do not be empty. Updating
            the Bounding box $\mathcal{BOX}_m = \mathcal{BOX}_{m-1} \bigcap \mathcal{B}_{m}$. For
            each dimension $i$ in $\mathcal{BOX}_m$, $U_i = min(U_i, \frac{v_1}{v_m}x_{mi}+
            \frac{v_1}{v_m}\epsilon)$, and $L_i = max(L_i , \frac{v_1}{v_m}x_{mi}-\frac{v_1}{v_m}\epsilon)$. Return True.
\end{enumerate}


% The method as below: assume the stream $i_{th}$ data
% $D_i = (x_{i1}, x_{i2}, ..., x_{in},t_i)$, the first data $D_0$ would
% be save as base point $Z=(z_{1}, z_{2}, ..., z_{n},t_z)$in the Method.
% As usual, the timestamp after basepoint would be set as mapping plane.
% We also need to record the intersect $S$ for find result lines.
% The first data $D_1$ would change into tolerance range $R_1$.
% initializing $S = R_1$. For each follow data$ D_i, i>1$, mapping $D_1$
% into $\hat{D_1}$ = $(\{\hat{x_{ij}}=z_j + \frac{x_{ij}-z_{j}}{t_i-t_z} \mid1\leqslant{j}\leqslant{n}\},t_i)$,
% and error margin after mapping should be $\frac{\epsilon}{t_i-t_z}$
% on $\hat{x_{ij}}, j={1...n}$. The tolerance range after R_i, intersect
% with S, if there is overlap, update S = S intersect R_i. If not, the base point
% would be transmited and select a point in S as base point with timestamp $t_{i-1}$.

% \todo{refector}
% At beginning of the algorithm, we need a base point (which we will call \textit{z}) and bounding box which has upper and lower bound for each parameter respectively.Then start the algorithm.
% \begin{enumerate}
%   \item Initialization: Get first data point, and set point \textit{z} equals to first data point. Get the second data point $(x_{21},...,x_{2n},t_2)$, and assign each upper bound $U_j = x_{2j}+\frac{\epsilon}{t_2-t_1}$ and lower bound $L_j = x_{2j}-\frac{\epsilon}{t_2-t_1}$ where $j=\{1...n\}$.
%   \item Get next data point, map the data point $(x_{i1},...,x_{in},t_i)$ into $(\{\hat{x_{ij}}=z_j + \frac{x_{ij}-z_{j}}{t_i-t_z} \mid1\leqslant{j}\leqslant{n}\},t_i)$.
%   \item For each parameter(dimension) \textit{j}, if upper bound $U_j$ smaller than $x_{ij}-\frac{\epsilon}{t_i-t_1}$ or lower bound $L_j$ bigger than $x_{ij}+\frac{\epsilon}{t_i-t_1}$, then \textbf{goto 5}, else $U_j = min(U_j, x_{ij}+\frac{\epsilon}{t_i-t_1})$, and $L_j = max(L_j, x_{ij}-\frac{\epsilon}{t_i-t_1})$.
%   \item Goto 2.
%   \item output \textit{z} data point.
%   \item Reset: set data point \textit{z} equals to center of the bounding box with time-stamp $t_{i-1}$, and $U_j=x_{ij}+\frac{\epsilon}{t_i-t_{i-1}}$, $L_j =x_{ij}-\frac{\epsilon}{t_i-t_{i-1}}$ with $j=\{1...n\}$.
%   \item Goto 2.
%   \item After all, output \textit{z} data point and center of bounding box respectively.
% \end{enumerate}

According the Implement, the infinity norm boils down to apply 1D LTC algorithm in each dimension respectively. Since, the intersection would be rectangle in 2D and cube in 3D, the corresponding max errors are $\frac{\epsilon}{\sqrt{2}}$ and $\frac{\epsilon}{\sqrt{3}}$. As we mentioned in Section 2, the infinity norm is more 'tolerant' than Euclidean norm. The max error between original data and reconstructed data certainly not greater than $\frac{\epsilon}{\sqrt{n}}$ where $n$ is dimension of data.
% for tolerance range, it depends on EPSILON and different type of distances for EPSILON.
% If we using Manhattan distance to create tolerance range, it would become rectangle in 2D and cube for 3D.
% But the tolerance range wold become disk or ball in 2D and 3D respectively, by using Euclidean distance.
\todo{maybe need a graphic}


% In practice, Multidimensional LTC is same as implementing LTC in each
% parameter respectively. But we did some changes in our method. Assume
% $(x_{i1}, x_{i2}, ..., x_{in},t_i)$ is the $i_{th}$ coming data with
% n-dimension, and a base data point Z  $(z_{1}, z_{2}, ...,
% z_{n},t_z)$~\cite{schoellhammer2004lightweight}. We map each coming
% data to $t_1$ time-stamp and create a bounding box for recording
% overlapping range which includes upper and lower bound for each
% dimension base on $t_1$. cause the time different between two adjacent
% data, so the $i_th$ data after mapping is $(\{\hat{x_{ij}}=z_j +
% \frac{x_{ij}-z_{j}}{t_i-t_z} \mid1\leqslant{j}\leqslant{n}\},t_i)$.
% After that updating tolerance range by adding designed error margin
% which after mapping $\frac{\epsilon}{t_i-t_z}$ on $\hat{x_{ij}},
% j={1...n}$ with Manhattan distance. checking if there is overlap
% between tolerance and bounding box.The algorithm is as follows.

% At beginning of the algorithm, we need a base point (which we will call
% \textit{z}) and bounding box which has upper and lower bound for each
% parameter respectively.Then start the algorithm.
% \begin{enumerate}
%   \item Initialization: Get first data point, and set point \textit{z} equals to first data point. Get the second data point $(x_{21},...,x_{2n},t_2)$, and assign each upper bound $U_j = x_{2j}+\frac{\epsilon}{t_2-t_1}$ and lower bound $L_j = x_{2j}-\frac{\epsilon}{t_2-t_1}$ where $j=\{1...n\}$.
%   \item Get next data point, map the data point $(x_{i1},...,x_{in},t_i)$ into $(\{\hat{x_{ij}}=z_j + \frac{x_{ij}-z_{j}}{t_i-t_z} \mid1\leqslant{j}\leqslant{n}\},t_i)$.
%   \item For each parameter(dimension) \textit{j}, if upper bound $U_j$ smaller than $x_{ij}-\frac{\epsilon}{t_i-t_1}$ or lower bound $L_j$ bigger than $x_{ij}+\frac{\epsilon}{t_i-t_1}$, then \textbf{goto 5}, else $U_j = min(U_j, x_{ij}+\frac{\epsilon}{t_i-t_1})$, and $L_j = max(L_j, x_{ij}-\frac{\epsilon}{t_i-t_1})$.
%   \item Goto 2.
%   \item output \textit{z} data point.
%   \item Reset: set data point \textit{z} equals to center of the bounding box with time-stamp $t_{i-1}$, and $U_j=x_{ij}+\frac{\epsilon}{t_i-t_{i-1}}$, $L_j =x_{ij}-\frac{\epsilon}{t_i-t_{i-1}}$ with $j=\{1...n\}$.
%   \item Goto 2.
%   \item After all, output \textit{z} data point and center of bounding box respectively.
% \end{enumerate}

\subsection{Multidimensional LTC with Euclidean distance}
In Euclidea norm, the representation of intersection is
volume. There is no specific model to record repersentation
with updating, so a list to save all balls $\mathcal{B}_j, 1\leqslant{j}\leqslant{m}$
is needed. In order to test weather a set of balls intersect,
the Helly's theorem could work, but the complexity of
this method is too high to work with embeded, low-power
systems which has limited CPU and RAM resource. So we
implement Euclidean norm by using a method which based
on plane sweep and binary search. Assume the list of balls $S_{m-1}$ contains balls from $\mathcal{B}_1$ to $\mathcal{B}_{m-1}$, and an temporary data point $output$ used for updated origin $(\tau^k, \xi^k)$. From the formula (1) in Section 3, the $(\mathcal{B}_j)_{j \in \llbracket 1, {m-1} \rrbracket}$ is a sequence of decreasing radius, so the larger balls that includes new coming ball $\mathcal{B}_{m-1}$ could be reomved from list $S_{m-1}$ so that reducing processing complexity of method. Therefore the list $S_{m-1}$ may not includes $m-1$ balls. Assume each $\Hat{\mathcal{B}} \in S$ which has corresponding center $\Hat{z}$ and radius $\Hat{\epsilon}$.
Let origin $(u^k_0, y^k_0) = (\tau_k, \xi_k)$, and for each data $(u_m^k, y_m^k)$ with diminsion $n$, defines $(v_m, z_m) = (u_m^k - \tau^k, y_m^k - \xi^k)$ where $z_m = \{ x_{mi} \mid 1\leqslant{i}\leqslant{n}\}$. The $\mathcal{B}_{m}$ should be one n-ball with center $\frac{v_1}{v_j}z_m$ and radius $\frac{v_1}{v_j}\epsilon$. The method is written in Algorithm~\ref{algo:isThereIntersection}.

\begin{algorithm}
    \begin{flushleft}
        \textbf{Input:} $S$, $\mathcal{B}_m$, $output$ \\
        \textbf{Output:} $true|false$\\
    \end{flushleft}
    \begin{algorithmic}[1]
        \State d = n \Comment{dimension index}
        \State $max_d = \frac{v_1}{v_m}(x_d + \epsilon)$
        \State $min_d = \frac{v_1}{v_m}(x_d - \epsilon)$
        \ForAll{$\Hat{\mathcal{B}}$ \textbf{in} $S$}
            \If{$\Hat{\mathcal{B}} \bigcap \mathcal{B}_m = $ \O }
                \State \textbf{return} false
            \ElsIf{$\mathcal{B}_m \bigcap \Hat{\mathcal{B}}  = \mathcal{B}_m $}
                \State remove $\Hat{\mathcal{B}}$ from $S$
                \State \textbf{continue}\\
            $max_d = \min(max_d, \Hat{x_d} + \Hat{\epsilon}$)\\
            $min_d = \max(min_d, \Hat{x_d} - \Hat{\epsilon}$)
            \EndIf
        \EndFor
        \State $\textbf{add } \mathcal{B}_m \textbf{ into } S$
        \If{$max_d < min_d$}
            \State \textbf{return} false
        \EndIf
        \State temp\_output = output \Comment{used for next operation}
        \If{Recursive$(min_d, max_d, n, p\_cp)$}
            \State $output = temp\_output$
            \State \textbf{return} true
        \Else
            \State \textbf{return} false
        \EndIf
    \end{algorithmic}
    \caption{isThereIntersection}
    \label{algo:isThereIntersection}
\end{algorithm}

% \begin{enumerate}
%     \item Initialzation: Let origin $(u^k_0, y^k_0) = (\tau_k, \xi_k)$.
%     \item For each data $(u_m^k, y_m^k)$ with diminsion $n$,
%             defines $(v_m, z_m) = (u_m^k - \tau^k, y_m^k - \xi^k)$ where $z_m =
%             \{ x_{mi} \mid 1\leqslant{i}\leqslant{n}\}$.
%     \item The $\mathcal{B}_{m}$ should be one n-ball with center
%             $\frac{v_1}{v_j}z_m$ and radius $\frac{v_1}{v_j}\epsilon$.
%     \item For each ball $\hat{\mathcal{B}}$ in list $S$, if $\hat{\mathcal{B}} \bigcap \mathcal{B}_m = \O$, return False. Else if
%     $\mathcal{B}_m\subset\hat{\mathcal{B}}$, remove $\hat{\mathcal{B}}$ from list $S$.
%     \item Select parameter p = n (which is ball's dimension), define $Max_{(p=n)} = \min_{j=1}^{S.length}{(\frac{v_1}{v_j}z_{(p=n)} + \frac{v_1}{v_j}\epsilon)}$, and $Min_{(p=n)} = \max_{j=1}^{S.length}{(\frac{v_1}{v_j}z_{(p=n)} - \frac{v_1}{v_j}\epsilon)}$. If $Max_{(p=n)} < Min_{(p=n)}$, return False.
%     \item Use binary search to select $mid_{(p=n)} = (Min_{(p=n)} + Max_{(p=n)})/2$
% \end{enumerate}

% In Euclidean distance version, we also map the coming data into same
% time-stamp $t_1$ the timestamp after base point. The difference with
% Manhattan distance version is that recording overlapping part with
% a post-designed model is difficult, which need retain several arcs in
% 2-dimension, or convex surfaces in 3-dimension. In our method, we will
% record all tolerance range for every mapped data which come from base
% data point until coming data point, in order to checking if there is
% intersection among them. For instance, in 3D-t, assume we have a the
% base point $Z = D_0$ and the tolerance range $R_1$ which is a ball
% as the intersection S. we need a list L to record mapped balls
% from $D_1$ to coming data. For each comming data $D_i, i>1$ mapping
% this data into $t_1$, and add it into list. Then the work is check out
% if all balls intersect, and form a public intersection.
% The method is based on plane sweep and binary search. At first, select one parameter,
% such as x-axis, finding a bounding range of all balls $B_i$ in x-axis.
% suppose $Max_bound_x = Min{1..n}{B_i.center.x + EPSILON/i}$, and
% $Min_bound_x = Max{1..n}{B_i.center.x - EPSILON/i}$. If there is overlap
% among balls, then the axis of the points inside intersection must located
% between Min_bound_x and Max_bound_x.
In the method, selecting dimension(n as default in our method) to calculate max point value and min point value of all n-balls in this dimension. If the max value bigger than min value, then there is no intersection. After that. In Recursive function Algorithm~\ref{algo:recursive}, using binary search select
median $mid_d$ of $max_d$ and $min_d$ in order to find bound range of dimension d-1 ($max_{d-1}$, $min_{d-1}$) over the plane $x_d = max_d$. If bound range of dimension $d-1$ is not empty, continue calling function \textbf{Recursive}($min_{d-1}$, $min_{d-1}$, $d-1$, $temp\_output$). Recursively calling this function, until getting result.
\todo{add recursive pseudo-code}
% Using binary search to select
% mid_x = (Min_bound_x + Max_bound_x)/2, then calculate a y-axis bounding
% with the plane x = mid_x. If the get bounding range in y-axis where
% $Max_bound_y > Min_bound_y$ and using binary search again to find mid_y,
% It means there is a line x=mid_x, Max_bound_y>= y >= Min_bound_y,
% parallels z-axis, could pass through all balls. Finally, in z-axis,
% computing bounding range, and If $Max_bound_z > Min_bound_z$, there
% must be one or more points inside all balls. The problem be solved.
% If the loop of binary search in x-axis overs, then there is no public
% intersection among balls, and need to transmit and update base point.
% In this method we need O(n*logn*logn) time complexity for binary search
% in x-axis and y-axis and traversal all balls in z-axis, also O(n) space
% complexity is needed for maintaining list. According this method,
% it could be extended into N-dimensional data. The main idea is to search,
% dimension by dimension, from a room or plane or line and get intersection
% point finally.

% \todo{below content is useful? }
% In the rest of this section, we describe examples for 2-dimensional and 3-dimensional version. After that, we extend the method for n-dimension.
% \begin{itemize}
% \item \textbf{2-dimensional LTC in Euclidean distance:}In this situation, the tolerance range after mapping is a disk. After initializing base data point \textit{z}, for each coming data point need to be mapped into a same time-stamp, and heck if there is a intersection amount disks in disks set and new mapped coming data. Therefore, a algorithm is need to determine whether \textit{n} disks intersect or not.
% \item \textbf{3-dimensional LTC in Euclidean distance:}In 3-dimension, cause of the preassigned error margin, the disks become balls with one extra axis. So In 3-dimension whether \textit{n} balls intersect need to be check.
% \end{itemize}

% At first, let us solve the \textit{n} disks intersect. We use a algorithm which is based on plane sweep and dichotomy. Assume a disk include center$(x,y)$ and radius $r$. The pseudo-code in Algorithm 1.

% \begin{algorithm}
%     \caption{whether disks intersect}
%     \begin{flushleft}
%         \textbf{Input:} $l$ - list of disks, $d$ - mapped coming disk, $p$ - point will be base point\\
%         \textbf{Output:} $true|false$  is there a intersection\\
%         \textbf{function} isIntersect$(l, d, p)$
%     \end{flushleft}
%     \begin{algorithmic}[1]
%         \State $tmp_list \gets null$
%         \State $max\_x\gets d.x+d.r$
%         \State $min\_x\gets d.x-d.r$

%         \ForAll{$old\_disk$ \textbf{in} $l$}
%             \If{$d \cap old\_disk = \emptyset$} % distance(d.center, old\_disk.center) > d.r + old\_disk.r
%                 \State \textbf{return} false
%             \ElsIf{$d \cap old\_disk \neq d$}
%                 \State {$\textbf{add } old\_disk \textbf{ into } tmp\_list$\\
%                         $max\_x\gets $ MIN($max\_x, old\_disk.x+old\_disk.r$)\\
%                         $min\_x\gets $ MAX($min\_x, old\_disk.x-old\_disk.r$)}
%             \EndIf
%         \EndFor
%         \State $\textbf{add } d \textbf{ into } tmp\_list$
%         \If{$max\_x < min\_x$}
%             \State \textbf{return} false
%         \EndIf
%         \While{$min\_x \leqslant max\_x$}
%             \State $mid \gets (min\_x + max\_x)/2$
%             \State $max\_y \gets +\infty$
%             \State $min\_y \gets -\infty$
%             \For{$i=1$ \textbf{to} $tmp\_list.length$}
%                 \State $P_1$ and $P_2$ are intersection points of $tmp\_list[i]$ and line $x=mid$, $(P1.y\geqslant P2.y)$
%                 \If{$P_1.y < max\_y$}
%                     \State $max\_y \gets p_1.y$
%                     \State $max\_index \gets i$
%                 \EndIf
%                 \If{$P_2.y > min\_y$}
%                     \State $min\_y \gets p_2.y$
%                     \State $min\_index \gets i$
%                 \EndIf
%             \EndFor
%             \If{$max\_y >= min\_y$}
%                 \State $p.x \gets mid$
%                 \State $p.y \gets (max\_y+min\_y)/2$
%                 \State $l \gets tmp\_list$
%                 \State \textbf{return} true
%             \EndIf
%             \State {Assume $P_d$ is the intersection between line of
%             centers from $tmp\_list[max\_index]$ and $tmp\_list[min\_index]$
%             , and their common chord}
%             \If{$P_d.x < mid$}
%                 \State $max\_y \gets mid-1$
%             \ElsIf{$P_d.x > mid$}
%                 \State $min\_y \gets mid+1$
%             \EndIf
%         \EndWhile
%         \Return false
%     \end{algorithmic}
% \end{algorithm}

% The main idea of the algorithm is, remove the bigger disk who contains mapped coming disk. It maybe increase Computational efficiency in the rest of algorithm, cause mapped coming disk is the smallest one than all in list of disks. Then we make a bounding range for x-axis and select a x-value $mid$ by using dichotomy method in order to calculate if a point $(y, mid)$ is included all disks.
In 2-dimension Euclidean norm, the complexity of this algorithm is $O(n)+O(n\log\epsilon) = O(n\log\epsilon)$. In 3-dimension, It needs $O(\log^2\epsilon)$ to determine $mid\_3, mid\_2$, and $O(n)$ to traverse all balls in list and calculate bounding range of dimension d=1. So 3-dimensional method need $O(n \log^2\epsilon)$ totally. If we extend this idea for n-dimension, the complexiy of this method is $O(n \log^{n}\epsilon)$ where n is the number of dimension.

% \begin{algorithm}
%     \begin{flushleft}
%         \textbf{Input:} $S$, $\mathcal{B}_m$, $output$
%         \textbf{Output:} $true|false$  is there a intersection\\
%         \textbf{function} isIntersect$(l, o, p)$
%     \end{flushleft}
%     \begin{algorithmic}[1]
%         \State $tmp_list \gets null$
%         \State $max \gets o.d\_n+o.r$
%         \State $min \gets o.d\_n-o.r$

%         \ForAll{$old\_obj$ \textbf{in} $l$}
%             \If{$o \cap old\_obj = \emptyset$}
%                 \State \textbf{return} false
%             \ElsIf{$o \cap old\_obj \neq o$}
%                 \State {$\textbf{add } old\_obj \textbf{ into } tmp\_list$\\
%                         $max\gets $ MIN($max, old\_obj.d\_n+old\_obj.r$)\\
%                         $min\gets $ MAX($min, old\_obj.d\_n-old\_obj.r$)}
%             \EndIf
%         \EndFor
%         \State $\textbf{add } o \textbf{ into } tmp\_list$
%         \If{$max < min$}
%             \State \textbf{return} false
%         \EndIf
%         $p\_cp \gets p$
%         \If{Recursive$(min, max, n, p\_cp)$}
%             \State $p \gets p\_cp$
%             \State \textbf{return} true
%         \Else
%             \State \textbf{return} false
%         \EndIf
%     \end{algorithmic}
%     \caption{isThereIntersection}
%     \label{algo:isThereIntersection}
% \end{algorithm}

\begin{algorithm}
    \begin{flushleft}
        \textbf{function} Recursive$(left, right, j, p)$  -- $j$ means $j_th$ dimension
    \end{flushleft}
    \begin{algorithmic}[1]
        \While{$left \leqslant right$}
            \State $mid \gets (left + right)/2$
            \State $max \gets +\infty$
            \State $min \gets -\infty$
            \For{$i=1$ \textbf{to} $tmp\_list.length$}
                \State $P_1$ and $P_2$ are intersection points of $tmp\_list[i]$ and line $d_{j}=mid$, $(P1.d_{j-1}\geqslant P2.d_{j-1})$
                \If{$P_1.d_{j-1} < max$}
                    \State $max \gets p_1.d_{j-1}$
                    \State $max\_index \gets i$
                \EndIf
                \If{$P_2.d_{j-1} > min$}
                    \State $min \gets p_2.d_{j-1}$
                    \State $min\_index \gets i$
                \EndIf
            \EndFor
            \If{$max >= min$}
                \State $p.d_j \gets mid$
                \If{Recursive$(min, max, j-1, p)$}
                    \State \textbf{return} true
                \EndIf
            \EndIf
            \State Assume $P_d$ is the intersection between common chord of two objects $tmp\_list[max\_index]$, $tmp\_list[min\_index]$ and their line of centers.
            \If{$P_d.d_j < mid$}
                \State $right \gets mid-1$
            \ElsIf{$P_d.d_j > mid$}
                \State $left \gets mid+1$
            \EndIf

        \EndWhile
        \Return false
    \end{algorithmic}
\end{algorithm}

\section{Results}

We conducted two experiments using Motsai's Neblina 
module\footnote{\url{ https://motsai.com/products/neblina}}, a system 
with a Nordic Semiconductor nRF52832 micro-controller, 64~KB of RAM, 
and Bluetooth Low Energy connectivity. The Neblina has a 3D 
accelerometer, 3D gyroscope, 3D magnetometer, and environmental sensors 
for humidity, temperature and pressure. It can function as an Inertial
Motion Unit, a Vertical Reference Unit, or an Attitude and Heading Reference
System. 

\subsection{Experiment 1: validation tests}

In this experiment we validated the behavior of our LTC extensions on a 
PC processing data acquired with the Neblina. We collected 2 data sets, 
``5-times biceps curl" and ``Mohammad lateral biceps curl". from the 
accelerometer with a 50Hz sampling rate (see 
Figure~\ref{fig:datasets}). 

We compressed the datasets with various values of $\epsilon$, using our 
2D (x and y) and 3D (x, y and z) implementations of LTC. On the Neblina,
the resolution of the accelerometer is about 20~mg (196~mm/$s^2$).
 We used a 
laptop computer with 16~GB of memory, and Intel i5-3210M CPU @ 2.50GHz 
Ã— 4, and Linux Fedora 27. We measured memory consumption using 
Valgrind's massif 
tool~\cite{nethercote2006building}\footnote{\url{http://valgrind.org}}, 
and processing time using \texttt{gettimeofday()} from the GNU C 
Library. 

Results are reported in Table~\ref{table:results-validation}. 
As expected, the maximum measured error remains lower than $\epsilon$ in 
all cases, and the compression ratio increases with $\epsilon$. 
%Compared to the infinity norm, The Euclidean norm reduces the 
%compression ratio by a factor of \pm, which was also expected.

    %~ \item Result: From the table 1 and table 2, The Max Error, 
    %~ Compression Radio and Time usage in both norm decrease when EPSILON 
    %~ become smaller. Comparing with different norm and same EPSILON, the 
    %~ infinity no, and few ndeed has more 'tolerance'. But when comparing 
    %~ using Infinity with $\epsilon=100/\sqrt{n}$, where n is number of 
    %~ dimension, to make sure the max error would not greater than 100, 
    %~ and Euclidean with $\epsilon$=100, the Euclidean norm has better 
    %~ compression radio. Although Infinity norm makes Max error larger, 
    %~ but it has less running time, and few fixed Memory usage according 
    %~ number of dimension. The Memory Usage of Euclidea is not fixed, 
    %~ because it need save $\mathcal{B}_i, i\subset\llbracket 1, m 
    %~ \rrbracket$ after origin $(u_0^k, y_0^k)$, costing lots of Memory. 
    %~ From the table 1 and 2, the Compression Radio in second data set is 
    %~ pretty better than first data set, because Mohammad Lateral bicep 
    %~ has more data than first dataset about 67.5 times. It is pointless 
    %~ to consider the different compression radio in this situation.

    %~ \item Conclusion: The infinity norm has higher compression radio 
    %~ than Euclidean norm with same $\epsilon$, and uses less running 
    %~ time and little memory. But Euclidean makes sure the Max Error 
    %~ between reconstructed data and original data under EPSILON. Both of 
    %~ norm would has lower Compression ratios when $\epsilon$ becomes 
    %~ smaller. 


\begin{table}

    \begin{subfigure}{\columnwidth}
    \centering
    \begin{tabular}{l|l|l|l|l}
    \hline
    \rowcolor{headcolor}
    Norm                       & \multicolumn{2}{c|}{Infinity} & \multicolumn{2}{c}{Euclidean}\\ \hline
    $\epsilon$                & 100          & 100/$\sqrt{2}$  & 100         & 100/$\sqrt{2}$ \\
    Max error              & 90.24       & 66.79           & 99.63       & 70.47          \\ 
    Compression ratio      & 29.03\%      & 20.88\%         & 25.12\%     & 19.57\%        \\ 
    Peak memory usage      & 80B          & 80B             & 432B        & 240B           \\ 
    Processing time        & 0.103ms      & 0.082ms         & 0.220ms     & 0.200ms        \\ \hline
    \end{tabular}
    \caption{5-times bicep curl (2D)}
    \end{subfigure}\\
    \begin{subfigure}{\columnwidth}
    \centering
    \begin{tabular}{l|l|l|l|l}
    \hline
    \rowcolor{headcolor}
    Norm               & \multicolumn{2}{c|}{Infinity} & \multicolumn{2}{c}{Euclidean} \\ \hline
    $\epsilon$             & 100        & 100/$\sqrt{2}$    & 100        & 100/$\sqrt{2}$    \\ 
    Max error              & 99.49     & 70.52             & 99.99      & 70.71             \\ 
    Compression ratio      & 60.17\%    & 51.59\%           & 57.75\%    & 48.86\%           \\ 
    Peak memory       & 80B        & 80B               & 2.1KB      & 1.3KB             \\ 
    Processing time      & 5.70ms     & 4.85ms            & 20.04ms    & 19.28ms           \\ \hline
    \end{tabular}
    \caption{Mohammad lateral biceps curl (2D)}
    \end{subfigure}\\    
    \begin{subfigure}{\columnwidth}
    \centering
    \begin{tabular}{l|l|l|l|l}
    \hline
    \rowcolor{headcolor}
                            & \multicolumn{2}{c|}{Infinity} & \multicolumn{2}{c}{Euclidean} \\ \hline
    $\epsilon$             & 100          & 100/$\sqrt{3}$  & 100         & 100/$\sqrt{3}$   \\ 
    Max error              & 100.17       & 60.38           & 99.94       & 56.89            \\ 
    Compression ratio      & 23.00\%      & 12.89\%         & 18.43\%     & 9.95\%           \\ 
    Peak memory            & 112B         & 112B            & 384B        & 256B             \\ 
    Processing time        & 0.119ms      & 0.094ms         & 0.230ms     & 0.200ms          \\ \hline
    \end{tabular}
    \caption{5-times biceps curl (3D)}
    \end{subfigure}\\
    \begin{subfigure}{\columnwidth}
    \centering
    \begin{tabular}{l|l|l|l|l}
    \hline
    \rowcolor{headcolor}
                              & \multicolumn{2}{c|}{Infinity} & \multicolumn{2}{c}{Euclidean} \\ \hline
    $\epsilon$                & 100        & 100/$\sqrt{3}$    & 100        & 100/$\sqrt{3}$    \\
    Max error                 & 97.54     & 56.00             & 99.99      & 57.73             \\
    Compression ratio         & 51.13\%    & 37.17\%           & 46.05\%    & 32.63\%           \\
    Peak memory & 112B        & 112B              & 4.9KB      & 3.0KB             \\
    Processing time           & 8.50ms     & 7.07ms            & 26.41ms    & 21.26ms           \\ \hline
    \end{tabular}
    \caption{Mohammad lateral biceps curl (3D)}
    \end{subfigure}
    \caption{Validation experiment}
    \label{table:results-validation}
\end{table}

\subsection{Experiment 2: energy reduction on the Neblina}


The capacity of battery in Neblina is 100mAh, and the battery usage 
would not change too much, because it contains others fusion method 
which will running when turning on the Neblina. In 50Hz sensor sampling 
rate, the average usage electric current for transmit accelerometer 
data is 2.0mA. It means Neblina could transmit accelerometer data 
constantly in 100/2.0=50 hours. when Neblina does not move, the average 
usage electric current using Infinity norm compression is 1.95mA. It 
just extend Neblina work life about 1.28 hours. But in 200Hz sampling 
rate. The electric before and after compression are 3.5 and 2.9 
respectively. The running time of Neblina would be extended about 5.9 
hours.

we wore Neblina on right wrist and
collected walking data and running data. 4000 data set was collected
from Neblina in 200Hz sampling rate. Since arm swing appearance
repeated, we just take 1000 data from it, and make data as cycle of
these 1000 data. Because 1000 data is produced in 5 seconds with 200Hz
sampling rate, we measured average energy usage in 5 seconds.


At first, we measured base energy usage in Neblina. The energy usage
without transmision is 2.515 mAh. It is 3.472mAh with transmision but
no compression method. From table 3 and table 4, The MaxError don't exceed EPSILON
and EPSILON*sqrt(Dimension) with LTC-Euclidean and LTC-Manhatan respectively.
And the average electric current increase with compression radio decreasing.
Energy usage by using both compression method are smaller than transmit
without compression. From the result, the activities which has low frequency
would gain larger compression radio.

The purpose of the compression method is compressing the number of
data needed to transmited, in order to extend Neblna's life cycle
by decreasing energy consumption. From table 3, for not vigorous exercise
such walking, with EPSILON equals 100, the life cycle could be extend
3(10\%)and 9(31\%) hours with LTC-Euclidean and LTC-Manhattan respectively.
And vigorous exercise, with same condition, it extends 5.7(19.7\%)
and 4.8(16.7\%) hours. So LTC-Euclidean and LTC-Manhattan are useful
to save devices Energy. At same time, The fidelity is guaranteed.

% \todo{ update }
% try using difference data set to measure compression radio, Error
% and energy usage. The numerical value presented below, is the
% average value in 5 seconds. The current electric current for Neblina
% without transmit is ***. we received 4000 data for human running and
% walking in 200Hz. Because of the limite RAM, for energy test, we only
% could save 400 static data into Neblina. We make 400 data run as a circle.
% % For walking, one time swing arm need 1.2 second. Every 400 data must have 1 arm swing.
% change epsilon in order to decrease compression radio, measuring energy consumption.

% \todo {running}
% If it's possible to save more static data?



% \begin{itemize}
%     \item Data set: human activities, running, walking, jogging and work behind desk.
%     \item Experiment Condition: work with neblina.
%     \item Result:
%     \item Conclusion:
% \end{itemize}



% The transform rate of Neblina is 50Hz.
% \\
% 5-times bicep curl from Neblina. It includes 613 data which produced in 12.28 seconds.
% \\
% Mohammad Lateral bicep data which include 41428, produced in proximate 14 minutes.

% \subsection{Compression ratios}

% \subsection{Errors}

% \subsection{Memory consumption}

\section{Conclusion}

\section*{Acknowledgement}


\begin{table}[]
    \begin{center}
    \caption{result of Experiment 2 with walking Data Set}
    \begin{tabular}{|l|l|l|l|l|l|l|}
    \hline
    DataSet           & \multicolumn{6}{l|}{walking}                                   \\\hline
    Distance          & \multicolumn{3}{l|}{Manhattan} & \multicolumn{3}{l|}{Euclidean} \\\hline
    Epsilon           & 100       & 20      & 10      & 100       & 20      & 10      \\\hline
    Max Error         & 158.793   & 31.828  & 16.763  & 153.141   & 31.763  & 15.898  \\\hline
    Compression Radio & 88.9\%    & 66.4\%  & 45.5\%  & 68.6\%    & 25.5\%  & 9.5\%   \\\hline
    Energy Usage      & 2.642     & 2.787   & 3.020   & 2.879     & 3.221   & 3.383   \\\hline
    \end{tabular}
    \end{center}
\end{table}

\begin{table}[]
    \begin{center}
    \caption{result of Experiment 2 with running Data Set}
    \begin{tabular}{|l|l|l|l|l|l|l|}
    \hline
    DataSet           & \multicolumn{6}{l|}{running}                                   \\\hline
    Distance          & \multicolumn{3}{l|}{Manhattan} & \multicolumn{3}{l|}{Euclidean} \\\hline
    Epsilon           & 100      & 20       & 10      & 100      & 20       & 10      \\\hline
    Max Error         & 99.995   & 19.984   & 9.979   & 99.995   & 19.989   & 9.959   \\\hline
    Compression Radio & 87.6\%   & 63.3\%   & 37.2\%  & 64.4\%   & 19.8\%   & 5.7\%   \\\hline
    Energy Usage      & 3.098    & 3.020    & 3.134   & 2.952    & 3.320    & 3.393   \\\hline
    \end{tabular}
    \end{center}
\end{table}

\bibliographystyle{IEEEtran}
\bibliography{IEEEabrv,biblio.bib}
\end{document}


\end{document}
