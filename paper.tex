\documentclass[10pt, conference, compsocconf]{IEEEtran}

% packages
\usepackage{algorithm}
\usepackage{algorithmicx} % Doc is at http://tug.ctan.org/macros/latex/contrib/algorithmicx/algorithmicx.pdf
\usepackage{algpseudocode}
\usepackage{amsfonts} % for R symbol (the set of real numbers)
\usepackage{color}
\usepackage{colortbl} % for \rowcolor
\usepackage[pdftex]{graphicx}
\usepackage{graphicx}
\usepackage[bookmarks=false]{hyperref}
\hypersetup{colorlinks=true,linkcolor=black,citecolor=black,filecolor=black,urlcolor=blue}
\usepackage{mathtools}
\usepackage{multirow}
\usepackage{stmaryrd} % for llbracket and rrbracket
\usepackage{subcaption}
\usepackage{nicefrac}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{stmaryrd}
\usepackage{amsthm}
\DeclarePairedDelimiter{\ceil}{\lceil}{\rceil}
\DeclarePairedDelimiter{\floor}{\lfloor}{\rfloor}

\algblock{Input}{EndInput}
\algnotext{EndInput}
\algblock{Output}{EndOutput}
\algnotext{EndOutput}
\newcommand{\Desc}[2]{\State \makebox[2em][l]{#1}#2}

% new commands
\newcommand{\todo}[1]{\marginpar{\parbox{18mm}{\flushleft\tiny\color{red}\textbf{TODO}:
      #1}}}
\newcommand{\note}[1]{
  \color{blue}\emph{[Note: #1]}
  \color{black}
}
\newcommand{\norm}[1]{\left\lVert#1\right\rVert}
\newtheorem*{theorem}{Theorem}
\definecolor{headcolor}{gray}{0.9}


\begin{document}

\title{A multi-dimensional extension of the Lightweight Temporal Compression method}

\author{Bo Li$^1$, Omid Sarbishei$^2$, Tristan Glatard$^1$\\
  $^1$ Department of Computer Science and Software Engineering, Concordia University, Montreal, QC, Canada \\
  $^2$ Motsai, Saint Bruno, QC, Canada.}
\maketitle

\begin{abstract}
Lightweight Temporal Compression
(LTC) is among the lossy stream
compression methods that provide the highest compression rate for the 
lowest CPU and memory consumption. As such, it is very well suited to 
compress data streams in energy-constrained systems such as connected 
objects. The current formulation of LTC, however, is one-dimensional 
while data acquired in connected objects is often multi-dimensional: for instance, 
accelerometers and gyroscopes usually measure variables along 
3 directions related by the actual movement of the sensor. In this 
paper, we investigate the extension of LTC to 
higher dimensions. First, we provide a formulation 
of the algorithm in an arbitrary vectorial space of dimension $n$. 
Then, we implement the algorithm for the infinity and Euclidean norms, 
in spaces of dimension 2D+t and 3D+t. We evaluate our implementation on 
3D acceleration streams acquired during human activities. 
Results show that the 3D implementation of LTC can save up to 20\% in 
energy consumption for low-paced activities, with a memory usage of about 100~B.
\end{abstract}

\section{Introduction}

With the recent technological advances in Internet of Things (IoT) 
applications, more than one billion connected objects are expected to 
be launched worldwide by 2025\footnote{\url{ 
https://www.statista.com/statistics/471264/iot-number-of-connected-devices-worldwide}} 
Power consumption is amongst the biggest challenges targeting connected 
objects, particularly in the industrial domains, where several sensing 
systems are commonly launched in the field to run for days or even 
weeks without being recharged. Typically, such devices use sensors to 
capture properties such as temperature or motion, and stream them to a 
host system over a radio transmission protocol such as Bluetooth 
Low-Energy (BLE). System designers aim to reduce the rate of data 
transmission as much as possible, as radio transmission is a 
power-hungry operation.

 Compression is a key technique to reduce the rate of radio 
 transmission.  While in several applications lossless compression 
 methods are more desirable than lossy compression techniques, in the 
 context of IoT and sensor data streams, the measured sensor data 
 intrinsically involves noise and measurement errors, which can 
 be treated as a configurable 
tolerance for a lossy compression algorithm. 

Resource-intensive lossy compression algorithms such as the ones based on 
polynomial interpolation, discrete cosine and Fourier transforms, or 
auto-regression methods~\cite{lu2010optimized} are not well-suited for 
energy-constrained systems, due to the limited memory available on 
these systems (typically a few KB), and the energy consumption 
associated with CPU usage. On such systems, compression algorithms need 
to find a trade-off between reducing network communications and 
increasing memory and CPU usage. As 
discussed in~\cite{zordan2014performance}, linear compression methods 
provide a very good compromise between these two factors, leading to 
substantial energy reduction.

% lossy vs lossless compression

% LTC summary
% define transmitted, received points, copmression ratio
The Lightweight Temporal Compression method 
(LTC~\cite{schoellhammer2004lightweight}) has been designed 
specifically for energy-constrained systems, initially sensor networks. 
It approximates data points by a piecewise linear function that 
guarantees an upper bound on the reconstruction error, and a reduced 
memory footprint in $\mathcal{O}(1)$. However, LTC has only been 
described for 1D streams, while streams acquired by connected objects, such as 
acceleration or gyroscopic data, are often multi-dimensional. 

In this paper, we extend LTC to dimension $n$. To do so, we propose an 
algebraic formulation of the algorithm that also yields a 
norm-independent expression of it. We implement our extension on 
Motsai's Neblina module, and we test it on 3D acceleration streams 
acquired during human exercises, namely biceps curling, walking and 
running. Our implementation of LTC is available as free software.

We assume that the stream consists of a sequence of data points 
received at uneven intervals. The compression algorithm 
\emph{transmits} fewer points than it receives. The transmitted points 
might be included in the stream, or computed from stream points. The 
\emph{compression ratio} is the ratio between the number of received 
points and the number of transmitted points. An application 
reconstructs the stream from the transmitted points: the 
\emph{reconstruction error} is the maximum absolute difference between 
a point of the reconstructed stream, and the corresponding 
point in the original stream.

\begin{figure}
\includegraphics[width=\columnwidth]{ltc.pdf}
\caption{Illustration of the LTC algorithm and notations used. Blue 
dots are received points, red dots are transmitted points. Dashed lines 
represent the high line and the low line at times when a point is 
transmitted.\vspace*{-0.3cm}}
\label{fig:ltc}
\end{figure}
\todo{show $\xi$ in the Figure}

% Outline
Section~\ref{sec:ltc} provides some background on the LTC algorithm, 
and formalizes the description initially proposed 
in~\cite{schoellhammer2004lightweight}. Section~\ref{sec:extension} 
presents our norm-independent extension to dimension $n$, and 
Section~\ref{sec:implementation} describes our implementation. 
Section~\ref{sec:results} reports on experiments to validate our 
implementation, and evaluates the impact of n-dimensional LTC on 
energy consumption of connected objects.


\section{Lightweight Temporal Compression}
\label{sec:ltc}

LTC approximates the data stream
by a piece-wise linear function of time, ensuring that the error at
every time-point is bounded by parameter $\epsilon$.

\subsection{Notations}

The algorithm receives a stream of data points $x_i$
at times $t_i$ ($i \in \mathbb{N}$), and it transmits a stream of data points $\xi_i$
at times $\tau_i$ ($i \in \mathbb{N}$). To simplify the notations, we assume that:
\begin{equation*}
\forall k \in \mathbb{N}, \  \exists ! i \in \mathbb{N} \  \tau_k = t_i
\end{equation*}
That is, transmission times coincide with reception times.
We define the \emph{shifted received points} as follows:
\begin{equation*}
\forall k \in \mathbb{N}\ , \forall j \in \mathbb{N^*},\ (u^k_j, y^k_j) = (t_{i+j}, x_{i+j}), 
\end{equation*}
where $i$ is such that $t_i = \tau_k$ and:
\begin{equation*}
\forall k \in \mathbb{N},\  (u^k_0, y^k_0) = (\tau_k, \xi_k).
\end{equation*}
This definition is such that $y^k_j$ is the $j^{th}$ data point received
after the $k^{th}$ transmission and $u^k_j$ is the corresponding timestamp.
Figure~\ref{fig:ltc} illustrates the notations and algorithm.

The LTC algorithm maintains two lines, the \emph{high line}, and the
\emph{low line} defined by (1) the latest transmitted point and (2) the
\emph{high point} (high line) and the \emph{low point} (low line). When
a point ($t_i$, $x_i$) is received, the high line is updated as
follows: if $x_i+\epsilon$ is below the high line then the high line is
updated to the line defined by the last transmitted point and ($t_i$,
$x_i+\epsilon$); otherwise, the high line is not updated. Likewise, the low line
is updated from $x_i-\epsilon$. Therefore, any line located between the
high line and the low line approximates the data points received since
the last transmitted point with an error bounded by $\epsilon$.

Using these notations, the original LTC algorithm can
be written as in Algorithm~\ref{algo:ltc}. For readability, we assume
that access to data points is blocking, i.e., the program will wait
until the points are available. We also assume that the content of
variable \texttt{tr} is transmitted after each assignment of this
variable. Function \texttt{line}, omitted for brevity, returns the
ordinate at abscissa $x$ (1st argument) of the line defined by the points
in its 2nd and 3rd arguments.

\begin{algorithm}
\begin{algorithmic}[1]
\Input
   \Desc{$(u^k_j, y^k_j)$}{$\quad \quad $Received data stream}
   \Desc{$\epsilon$}{$\quad \quad$Error bound}
\EndInput
\Output
   \Desc{tr}{Transmitted points}
\EndOutput
\State tr = $(u^0_0, y^0_0)$ \Comment{last transmitted point}
\State k = 0 ; j = 1
\State (lp, hp) = ($y^1_1 - \epsilon$, $y^1_1 + \epsilon$) \Comment{low and high points}

\While{True} \Comment{Process received points as they come}
    \State j += 1
    \State new\_lp = max($y^k_j-\epsilon$, line($u^k_j$, tr, ($u^k_{j-1}$, lp)))
    \State new\_hp = max($y^k_j+\epsilon$, line($u^k_j$, tr, ($u^k_{j-1}$, hp)))
    \If{new\_lp $<$ new\_hp} \Comment{Keep compressing}
        \State (lp, hp) = (new\_lp, new\_hp)
        \State \textbf{continue}
    \EndIf
    \State tr = $(u^k_j, (lp+hp)/2)$
    \Comment{Transmit point}
    \State (lp, hp) = ($y^k_j-\epsilon$, $y^k_j+\epsilon$)
    \State k += 1
    \State j = 0
\EndWhile
\end{algorithmic}
\caption{Original LTC algorithm, adapted from~\cite{schoellhammer2004lightweight}.}
\label{algo:ltc}
\end{algorithm}


 %~ is one of the linear estimation
%~ method to compress data. It is The first-order interpolator with two
%~ degrees of freedom (FOI-2DF) which is mentioned in
%~ ~\cite{jalaleddine1990ecg}.  Most of linear estimation method need a
%~ predefined parameter which is the error margin (let's call $\epsilon$)
%~ in order to guarantee the difference between estimation and original
%~ data within a scope.

%~ \todo{rewrite LTC algorithm, and there is a DP version of LTC maybe should also be written}

\section{Extension to dimension $n$}
\label{sec:extension}
In this section we provide a norm-independent formulation of LTC in
dimension $n$. By $n$ we refer to the dimension of the data points
$x_i$. To handle time, LTC actually operates in dimension
$n+1$.

\subsection{Preliminary comments}

We note that the formulation of LTC in~\cite{schoellhammer2004lightweight} relies on
the intersection of \emph{convex cones} in dimension $n+1$. For $n=1$, it
corresponds to the intersection of triangles, which can efficiently be
computed by maintaining boundary lines, as detailed previously. In higher dimension, however, cone intersections are not so
straighforward, due to the fact that the intersection between cones
may not be a cone.

To address this issue, we formulate LTC as an intersection test between
\emph{balls} of dimension $n$, that is, segments for $n=1$, disks for
$n=2$, etc. Balls are defined from the \emph{norm} used in
the vector space of data points. For $n=1$, the choice of the norm does
not really matter, as all p-norms and the infinity norm are identical.
In dimension $n$, however, norm selection will be critical.

\subsection{Algebraic formulation of LTC}

\subsubsection{Definitions}

Let $(u_0^k, y_0^k) \in \mathbb{R}^{n+1}$ be the latest transmitted point. For convenience, all the subsequent points will be
expressed in the orthogonal space with origin $(\tau_k, \xi_k)$. We denote by $(v_j, z_j)_{j \in \llbracket 0, m \rrbracket}$ such points:
\begin{equation*}
\forall j \leq m,\  (v_j, z_j) = (u_j^k - \tau_k, y_j^k - \xi_k)
\end{equation*}
We define $\mathcal{B}_j$ as the ball of $\mathbb{R}^n$ of center $\frac{v_1}{v_j}z_j$ and of radius
$\frac{v_1}{v_j}\epsilon$:
\begin{equation*}
\mathcal{B}_j = \left\{ z \in \mathbb{R}^n,\  \norm{z-\frac{v_1}{v_j}z_j} \leq \frac{v_1}{v_j}\epsilon \right\}
\end{equation*}

\subsubsection{LTC property}

We formulate the \emph{LTC property} as follows:
\begin{equation*}
\exists z \in \mathbb{R}^n, \ \forall j \in \llbracket 1, m \rrbracket, \norm{\frac{v_j}{v_1}z-z_j} \leq \epsilon.
\end{equation*}
The original LTC algorithm ensures that the LTC property is
verified between each transmission. Indeed, all the data points
$z$ such that $(v_1, z)$ is between the high line and the low line
verify the property. Line 9 in Algorithm~\ref{algo:ltc} guarantees that
such a point exists.

The LTC property can be re-written as follows:
\begin{equation*}
\exists z \in \mathbb{R}^n, \ \forall j \in \llbracket 1, m \rrbracket, \norm{z-\frac{v_1}{v_j}z_j} \leq \frac{v_1}{v_j}\epsilon
\end{equation*}
that is:
\begin{equation}
\bigcap_{j=1}^m \mathcal{B}_j \neq \O
\label{eq:ltc-property}
\end{equation}
Note that $(\mathcal{B}_j)_{j \in \llbracket 1, m \rrbracket}$ is a sequence
of balls of strictly decreasing radius, since $v_j > v_1$.

\subsection{Algorithm}

The LTC algorithm generalized to dimension $n$ tests that the LTC 
property in Equation~\ref{eq:ltc-property} is verified after each reception of a data 
point. It is written in Algorithm~\ref{algo:general-ltc}.
\begin{algorithm}
\begin{algorithmic}[1]
\Input
   \Desc{$(u^k_j, y^k_j)$}{$\quad \quad $Received data stream}
   \Desc{$\epsilon$}{$\quad \quad$Error bound}
\EndInput
\Output
   \Desc{tr}{Transmitted points}
\EndOutput

\State tr = ($\tau, \xi$) = ($u^0_0, y^0_0$)
\State k = 0 ; j = 0
\While{True}
    \State j += 1
    \State ($v_j, z_j$) = ($u_j^k - \tau, y_j^k - \xi$)
    \If{$\bigcap_{l=1}^j{\mathcal{B}_l} \neq \O$}
        \State \textbf{continue}
    \EndIf
    \State Pick $z$ in $\bigcap_{l=1}^{j-1}{\mathcal{B}_j}$
    \State tr = ($\tau$, $\xi$) = ($u^k_{j-1}, z$)
    \State k += 1
    \State j = 0
\EndWhile
\end{algorithmic}
\caption{Generalized LTC}
\label{algo:general-ltc}
\end{algorithm}

\todo{Check algorithms (reimplement)}

\subsection{Ball intersections}

Although Algorithm~\ref{algo:general-ltc} looks simple, one should not
overlook the fact that there is no good general algorithm to test
whether a set of balls intersect. The best general algorithm we could find
so far relies on Helly's theorem which is formulated as follows~\cite{helly1923mengen}:
\begin{theorem}
Let $\left\{ X_i \right\}_{i \in \llbracket 1, m \rrbracket}$ be a collection of convex subsets of $\mathbb{R}^n$. If the intersection of every $n+1$
subsets is non-empty, then the whole collection has an non-empty intersection.
\end{theorem}
\noindent This theorem leads to an algorithm of complexity ${m \choose n+1}$ which is
not usable in resource-constrained environment.

The only feasible algorithm that we found is norm-specific. It
maintains a representation of the intersection
$\bigcap_{j=1}^{m}{\mathcal{B}_j}$ which is updated at every iteration.
The intersection tests can then be done in constant time. However,
updating the representation of the intersection may be costly
depending on the norm used. For the infinity norm, the representation
is a rectangular cuboid which is straightforward to update by
intersection with an n-ball, since the n-ball is a rectangular cuboid too.
For the Euclidean norm, the representation is a volume with no particular property,
which is more costly to maintain.

\subsection{Effect of the norm}

As mentioned before, norm selection in $\mathbb{R}^n$ has a critical
impact on the compression error and ratio. To appreciate this effect,
let us consider the comparison between the infinity norm and the
Euclidean norm in dimension 2. By comparing the unit disk to a
square of side 2, we obtain that the compression ratio of a random stream would
be $\frac{4}{\pi}$ times larger with the infinity norm than with the 
Euclidean norm. In 3D, this ratio would be $\frac{6}{\pi}$. Conversely, 
a compression error bounded by $\epsilon$ with the infinity norm 
corresponds to a compression error of $\frac{\epsilon}{\sqrt{n}}$ with 
the Euclidean norm. Unsurprisingly, the
infinity norm is more 'tolerant' than the Euclidean norm.

It should also be noted that using the infinity norm in dimension $n$ 
boils down to the use of the 1D LTC algorithm independently in each 
dimension, since a data point will be transmitted as soon as the linear 
approximation doesn't hold in any of the dimensions.

\section{Implementation}
\label{sec:implementation}
Our code is available at ... (make a clean release!)

\todo{explain difference between exp 1 and exp 2 for euclidean}

%~ As reference above, the norm selection is importance. In this section,
%~ we show the implementation of infinity norm and Euclidean norm.

%~ \subsection{Multidimensional LTC with Manhattan distance}
%~ In section 2, we can know that the repersentation of intersection
%~ is a rectangular cuboid. So only one bounding box with n-dimensional
%~ is needed to represent the intersection. Assume a bounding box
%~ $\mathcal{BOX}_{m-1} = \bigcap_{j=1}^{m-1}{\mathcal{B}_{m-1}}$ which
%~ has Upper bound $U_i$ and lower Bound $L_i$ for each dimension
%~ $i$. So the fomular (1) in $6_{th}$ line of Algorithm 2 would
%~ be changed to $\mathcal{BOX}_{m-1} \bigcap \mathcal{B}_{m} \neq \O $. The Bounding box
%~ would be updated when this formular return true.
%~ Otherwise return false.
%~ \begin{enumerate}
    %~ \item Initialization: Let origin $(u^k_0, y^k_0) = (\tau_k, \xi_k)$.
    %~ \item For each data $(u_m^k, y_m^k)$ with diminsion $n$,
            %~ defines $(v_m, z_m) = (u_m^k - \tau^k, y_m^k - \xi^k)$ where $z_m =
            %~ \{ x_{mi} \mid 1\leqslant{i}\leqslant{n}\}$.
    %~ \item The $\mathcal{B}_{m}$ should be a ractangular cube with center
            %~ $\frac{v_1}{v_m}z_m$ and weight $\frac{v_1}{v_m}\epsilon$.
    %~ \item For each dimension $i$, box upper bound $U_i < \frac{v_1}{v_m}x_{mi}+
            %~ \frac{v_1}{v_m}\epsilon$ OR lower bound $L_i > \frac{v_1}{v_m}x_{mi}-\frac{v_1}{v_m}\epsilon$, return False.
    %~ \item Otherwise, the intersection do not be empty. Updating
            %~ the Bounding box $\mathcal{BOX}_m = \mathcal{BOX}_{m-1} \bigcap \mathcal{B}_{m}$. For
            %~ each dimension $i$ in $\mathcal{BOX}_m$, $U_i = min(U_i, \frac{v_1}{v_m}x_{mi}+
            %~ \frac{v_1}{v_m}\epsilon)$, and $L_i = max(L_i , \frac{v_1}{v_m}x_{mi}-\frac{v_1}{v_m}\epsilon)$. Return True.
%~ \end{enumerate}


%~ According the Implement, the infinity norm boils down to apply 1D LTC algorithm in each dimension respectively. Since, the intersection would be rectangle in 2D and cube in 3D, the corresponding max errors are $\frac{\epsilon}{\sqrt{2}}$ and $\frac{\epsilon}{\sqrt{3}}$. As we mentioned in Section 2, the infinity norm is more 'tolerant' than Euclidean norm. The max error between original data and reconstructed data certainly not greater than $\frac{\epsilon}{\sqrt{n}}$ where $n$ is dimension of data.

%~ \todo{maybe need a graphic}

%~ \subsection{Multidimensional LTC with Euclidean distance}
%~ In Euclidea norm, the representation of intersection is
%~ volume. There is no specific model to record repersentation
%~ with updating, so a list to save all balls $\mathcal{B}_j, 1\leqslant{j}\leqslant{m}$
%~ is needed. In order to test weather a set of balls intersect,
%~ the Helly's theorem could work, but the complexity of
%~ this method is too high to work with embeded, low-power
%~ systems which has limited CPU and RAM resource. So we
%~ implement Euclidean norm by using a method which based
%~ on plane sweep and binary search. Assume the list of balls $S_{m-1}$ contains balls from $\mathcal{B}_1$ to $\mathcal{B}_{m-1}$, and an temporary data point $output$ used for updated origin $(\tau^k, \xi^k)$. From the formula (1) in Section 3, the $(\mathcal{B}_j)_{j \in \llbracket 1, {m-1} \rrbracket}$ is a sequence of decreasing radius, so the larger balls that includes new coming ball $\mathcal{B}_{m-1}$ could be reomved from list $S_{m-1}$ so that reducing processing complexity of method. Therefore the list $S_{m-1}$ may not includes $m-1$ balls. Assume each $\Hat{\mathcal{B}} \in S$ which has corresponding center $\Hat{z}$ and radius $\Hat{\epsilon}$.
%~ Let origin $(u^k_0, y^k_0) = (\tau_k, \xi_k)$, and for each data $(u_m^k, y_m^k)$ with diminsion $n$, defines $(v_m, z_m) = (u_m^k - \tau^k, y_m^k - \xi^k)$ where $z_m = \{ x_{mi} \mid 1\leqslant{i}\leqslant{n}\}$. The $\mathcal{B}_{m}$ should be one n-ball with center $\frac{v_1}{v_j}z_m$ and radius $\frac{v_1}{v_j}\epsilon$. The method is written in Algorithm~\ref{algo:isThereIntersection}.

%~ \begin{algorithm}
    %~ \begin{flushleft}
        %~ \textbf{Input:} $S$, $\mathcal{B}_m$, $output$ \\
        %~ \textbf{Output:} $true|false$\\
    %~ \end{flushleft}
    %~ \begin{algorithmic}[1]
        %~ \State d = n \Comment{dimension index}
        %~ \State $max_d = \frac{v_1}{v_m}(x_d + \epsilon)$
        %~ \State $min_d = \frac{v_1}{v_m}(x_d - \epsilon)$
        %~ \ForAll{$\Hat{\mathcal{B}}$ \textbf{in} $S$}
            %~ \If{$\Hat{\mathcal{B}} \bigcap \mathcal{B}_m = $ \O }
                %~ \State \textbf{return} false
            %~ \ElsIf{$\mathcal{B}_m \bigcap \Hat{\mathcal{B}}  = \mathcal{B}_m $}
                %~ \State remove $\Hat{\mathcal{B}}$ from $S$
                %~ \State \textbf{continue}\\
            %~ $max_d = \min(max_d, \Hat{x_d} + \Hat{\epsilon}$)\\
            %~ $min_d = \max(min_d, \Hat{x_d} - \Hat{\epsilon}$)
            %~ \EndIf
        %~ \EndFor
        %~ \State $\textbf{add } \mathcal{B}_m \textbf{ into } S$
        %~ \If{$max_d < min_d$}
            %~ \State \textbf{return} false
        %~ \EndIf
        %~ \State temp\_output = output \Comment{used for next operation}
        %~ \If{Recursive$(min_d, max_d, n, p\_cp)$}
            %~ \State $output = temp\_output$
            %~ \State \textbf{return} true
        %~ \Else
            %~ \State \textbf{return} false
        %~ \EndIf
    %~ \end{algorithmic}
    %~ \caption{isThereIntersection}
    %~ \label{algo:isThereIntersection}
%~ \end{algorithm}

%~ In the method, selecting dimension(n as default in our method) to calculate max point value and min point value of all n-balls in this dimension. If the max value bigger than min value, then there is no intersection. After that. In Recursive function Algorithm~\ref{algo:recursive}, using binary search select
%~ median $mid_d$ of $max_d$ and $min_d$ in order to find bound range of dimension d-1 ($max_{d-1}$, $min_{d-1}$) over the plane $x_d = max_d$. If bound range of dimension $d-1$ is not empty, continue calling function \textbf{Recursive}($min_{d-1}$, $min_{d-1}$, $d-1$, $temp\_output$). Recursively calling this function, until getting result.
%~ \todo{add recursive pseudo-code}

%~ In 2-dimension Euclidean norm, the complexity of this algorithm is $O(n)+O(n\log\epsilon) = O(n\log\epsilon)$. In 3-dimension, It needs $O(\log^2\epsilon)$ to determine $mid\_3, mid\_2$, and $O(n)$ to traverse all balls in list and calculate bounding range of dimension d=1. So 3-dimensional method need $O(n \log^2\epsilon)$ totally. If we extend this idea for n-dimension, the complexiy of this method is $O(n \log^{n}\epsilon)$ where n is the number of dimension.

%~ \begin{algorithm}
    %~ \begin{flushleft}
        %~ \textbf{function} Recursive$(left, right, j, p)$  -- $j$ means $j_th$ dimension
    %~ \end{flushleft}
    %~ \begin{algorithmic}[1]
        %~ \While{$left \leqslant right$}
            %~ \State $mid \gets (left + right)/2$
            %~ \State $max \gets +\infty$
            %~ \State $min \gets -\infty$
            %~ \For{$i=1$ \textbf{to} $tmp\_list.length$}
                %~ \State $P_1$ and $P_2$ are intersection points of $tmp\_list[i]$ and line $d_{j}=mid$, $(P1.d_{j-1}\geqslant P2.d_{j-1})$
                %~ \If{$P_1.d_{j-1} < max$}
                    %~ \State $max \gets p_1.d_{j-1}$
                    %~ \State $max\_index \gets i$
                %~ \EndIf
                %~ \If{$P_2.d_{j-1} > min$}
                    %~ \State $min \gets p_2.d_{j-1}$
                    %~ \State $min\_index \gets i$
                %~ \EndIf
            %~ \EndFor
            %~ \If{$max >= min$}
                %~ \State $p.d_j \gets mid$
                %~ \If{Recursive$(min, max, j-1, p)$}
                    %~ \State \textbf{return} true
                %~ \EndIf
            %~ \EndIf
            %~ \State Assume $P_d$ is the intersection between common chord of two objects $tmp\_list[max\_index]$, $tmp\_list[min\_index]$ and their line of centers.
            %~ \If{$P_d.d_j < mid$}
                %~ \State $right \gets mid-1$
            %~ \ElsIf{$P_d.d_j > mid$}
                %~ \State $left \gets mid+1$
            %~ \EndIf

        %~ \EndWhile
        %~ \Return false
    %~ \end{algorithmic}
%~ \end{algorithm}

\section{Experiments and Results}
\label{sec:results}

\todo{talk about data shift}

We conducted two experiments using Motsai's Neblina 
module\footnote{\url{ https://motsai.com/products/neblina}}, a system 
with a Nordic Semiconductor nRF52832 micro-controller, 64~KB of RAM, 
and Bluetooth Low Energy connectivity. Neblina has a 3D 
accelerometer, a 3D gyroscope, a 3D magnetometer, and environmental 
sensors for humidity, temperature and pressure. The platform is 
equipped with sensor fusion algorithms for 3D orientation tracking and 
a machine learning engine for complex motion analysis and motion 
pattern recognition~\cite{sarbishei2016accuracy}. Neblina has a 
battery of 100mAh; its average consumption is 2.52~mA when using 
accelerometer and gyroscope sensors at 200~Hz but without radio 
transmission, and 3.47~mA with radio transmission, leading to an 
autonomy of 39.7~h without transmission and 28.8~h with transmission. 

\subsection{Experiment 1: validation}

\todo{In all of our experiments, the board was attached to the lower 
right arm as shown in Fig. X. (Put a picture of Neblina in a box worn 
on the wrist)}

In this experiment, we validated the behavior of our LTC extension on a 
PC using data acquired with Neblina. We collected two 3D 
accelerometer time series, a short one and a longer one, acquired 
on two different subjects performing biceps curl, with a 50Hz sampling rate (see 
Figure~\ref{fig:datasets-1}). It should be noted that the longer dataset also has 
a higher amplitude, most likely due to differences between subjects.

\begin{figure*}
\begin{subfigure}{2\columnwidth}
\includegraphics[width=0.33\columnwidth]{figures/5-time-bicep-curl-plot-x.pdf}
\includegraphics[width=0.33\columnwidth]{figures/5-time-bicep-curl-plot-y.pdf}
\includegraphics[width=0.33\columnwidth]{figures/5-time-bicep-curl-plot-z.pdf}
\caption{Short biceps curl}
\end{subfigure}
\begin{subfigure}{2\columnwidth}
\includegraphics[width=0.33\columnwidth]{figures/Mohammad-bicep-curl-plot-x.pdf}
\includegraphics[width=0.33\columnwidth]{figures/Mohammad-bicep-curl-plot-y.pdf}
\includegraphics[width=0.33\columnwidth]{figures/Mohammad-bicep-curl-plot-z.pdf}
\caption{Long biceps curl} 
\end{subfigure}
\caption{Time series used in Experiment 1}
\label{fig:datasets-1}
\end{figure*}

We compressed the time series with various values of $\epsilon$, using 
our 2D (x and y) and 3D (x, y and z) implementations of LTC. On 
Neblina, the raw uncalibrated accelerometer data corresponds to errors 
around 20~mg (196~mm/$s^2$). \todo{Check g units} We used a 
laptop computer with 16~GB of RAM, an Intel i5-3210M CPU @ 2.50GHz 
Ã— 4, and Linux Fedora 27. We measured memory consumption using 
Valgrind's massif 
tool~\cite{nethercote2006building}\footnote{\url{http://valgrind.org}}, 
and processing time using \texttt{gettimeofday()} from the GNU C 
Library. 

Results are reported in Table~\ref{table:results-validation}. 
As expected, the compression ratio increases with $\epsilon$, and the 
maximum measured error remains lower than $\epsilon$ in all cases. The 
maximum is reached most of the time on these time series.

\paragraph{Inifinity vs Euclidean norms}
Compared to the infinity norm, the average ratio between the compression ratios obtained
with the infinity and Euclidean norm is 1.08 for 2D data, and 1.20 
for 3D data. These ratios are lower than the theoretical values of 
$\frac{4}{\pi}$ in 2D and $\frac{6}{\pi}$ in 3D, which are obtained for 
random-uniform signals. Unsurprisingly, the infinity norm surpasses the 
Euclidean norm in terms of resource consumption. Memory-wise, the 
infinity norm requires a constant amount of 80~B, used to store the 
intersection of n-balls. The Euclidean norm, however, uses up to 4.9~KB of memory 
for the Long time series in 3D with $\epsilon=100$. More importantly, 
the amount of required memory increases for more complex time series 
(Long vs Short), and it also increases with larger 
values of $\epsilon$. Similar observations are made for the processing 
time, with values ranging from 0.2~ms for the simplest time series and 
smallest $\epsilon$, to 26.4~ms for the most complex time series and 
largest $\epsilon$. 

\paragraph{2D vs 3D}
For a given $\epsilon$ of 100 and a given time series, the compression 
ratios are always higher in 2D than in 3D. It makes sense since the 
probability for the signal to deviate from a straight line 
approximation is higher in 3D than it is in 2D. Besides, resource 
consumption is higher in 3D than in 2D: for the infinity norm, 3D 
consumes 1.4 times more memory than 2D (1.6 times on average for 
Euclidean norm), and processing time is 1.3 longer on average (1.18 on 
average for Euclidean norm).

\begin{table}
    \begin{subfigure}{\columnwidth}
    \centering
    \begin{tabular}{l|l|l|l|l}
    \hline
    \rowcolor{headcolor}
                           & \multicolumn{2}{c|}{Infinity} & \multicolumn{2}{c}{Euclidean}\\ \hline
    $\epsilon$                & 100          & 100/$\sqrt{2}$  & 100         & 100/$\sqrt{2}$ \\
    Max error              & 99.88       & 70.68           & 99.63       & 70.47          \\ 
    Compression ratio      & 29.03\%      & 20.88\%         & 25.12\%     & 19.57\%        \\ 
    Peak memory            & 80~B          & 80~B             & 432~B        & 240~B           \\ 
    Processing time        & 0.103~ms      & 0.082~ms         & 0.220~ms     & 0.200~ms        \\ \hline
    \end{tabular}
    \caption{Short biceps curl (2D)}
    \end{subfigure}\\
    \begin{subfigure}{\columnwidth}
    \centering
    \begin{tabular}{l|l|l|l|l}
    \hline
    \rowcolor{headcolor}
                   & \multicolumn{2}{c|}{Infinity} & \multicolumn{2}{c}{Euclidean} \\ \hline
    $\epsilon$             & 100        & 100/$\sqrt{2}$    & 100        & 100/$\sqrt{2}$    \\ 
    Max error              & 99.99     & 70.71             & 99.99      & 70.71             \\ 
    Compression ratio      & 60.17\%    & 51.59\%           & 57.75\%    & 48.86\%           \\ 
    Peak memory       & 80~B        & 80~B               & 2.1~KB      & 2.5~KB             \\ 
    Processing time      & 5.70~ms     & 4.85~ms            & 20.04~ms    & 19.28~ms           \\ \hline
    \end{tabular}
    \caption{Long biceps curl (2D)}
    \end{subfigure}\\    
    \begin{subfigure}{\columnwidth}
    \centering
    \begin{tabular}{l|l|l|l|l}
    \hline
    \rowcolor{headcolor}
                            & \multicolumn{2}{c|}{Infinity} & \multicolumn{2}{c}{Euclidean} \\ \hline
    $\epsilon$             & 100          & 100/$\sqrt{3}$  & 100         & 100/$\sqrt{3}$   \\ 
    Max error              & 99.88       & 57.41           & 99.94       & 56.89            \\ 
    Compression ratio      & 23.00\%      & 12.89\%         & 18.43\%     & 9.95\%           \\ 
    Peak memory            & 112~B         & 112~B            & 384~B        & 256~B             \\ 
    Processing time        & 0.119~ms      & 0.094~ms         & 0.230~ms     & 0.200~ms          \\ \hline
    \end{tabular}
    \caption{Short biceps curl (3D)}
    \end{subfigure}\\
    \begin{subfigure}{\columnwidth}
    \centering
    \begin{tabular}{l|l|l|l|l}
    \hline
    \rowcolor{headcolor}
                              & \multicolumn{2}{c|}{Infinity} & \multicolumn{2}{c}{Euclidean} \\ \hline
    $\epsilon$                & 100        & 100/$\sqrt{3}$    & 100        & 100/$\sqrt{3}$    \\
    Max error                 & 99.96     & 57.73             & 99.99      & 57.73             \\
    Compression ratio         & 51.13\%    & 37.17\%           & 46.05\%    & 32.63\%           \\
    Peak memory & 112~B        & 112~B              & 4.9~KB      & 3.0~KB             \\
    Processing time           & 8.50~ms     & 7.07~ms            & 26.41~ms    & 21.26~ms           \\ \hline
    \end{tabular}
    \caption{Long biceps curl (3D)}
    \end{subfigure}
    \caption{Results from Experiment 1}
    \label{table:results-validation}
\end{table}

\begin{figure*}
\begin{subfigure}{2\columnwidth}
\includegraphics[width=0.33\columnwidth]{./figures/walking-1000-x.pdf}
\includegraphics[width=0.33\columnwidth]{./figures/walking-1000-y.pdf}
\includegraphics[width=0.33\columnwidth]{./figures/walking-1000-z.pdf}
\caption{Walking}
\end{subfigure}
\begin{subfigure}{2\columnwidth}
\includegraphics[width=0.33\columnwidth]{./figures/running-1000-x.pdf}
\includegraphics[width=0.33\columnwidth]{./figures/running-1000-y.pdf}
\includegraphics[width=0.33\columnwidth]{./figures/running-1000-z.pdf}
\caption{Running}
\end{subfigure}
\caption{Time series used in Experiment 2}
\label{fig:datasets-2}
\end{figure*}
\subsection{Experiment 2: impact on energy consumption}

We measured the energy consumption of Neblina
for different values of $\epsilon$ and norms, to quantify the 
added-value of LTC for this system. 

We acquired two 3D accelerometer time series at 200Hz corresponding to 
common human activities: walking and running (see Figure~\ref{fig:datasets-2}). In both cases, the 
subject was wearing Neblina on their right wrist.
We collected 1,000 data points for each activity, corresponding to 
5 seconds of activity.

We measured energy consumption associated with the transmission of 
these time series, for different norms and $\epsilon$ values. To do so, 
we `replayed' the time series by loading them as a byte array in 
Neblina, and we measured the current every 500~ms.

 Results are reported in Table~\ref{table:results-energy}. For a given 
 $\epsilon$ and norm, the compression ratio is larger for walking than 
 for running. The ratio of saved energy is relative to the reference 
 current of 3.47~mA measured when Neblina transmits data without 
 compression. In all cases, activating compression saves energy. The 
 reduction in energy consumption behaves as the compression ratio: it 
 increases with $\epsilon$,
it is higher for the infinity norm than for the Euclidean, and it is
higher for the walking activity than for running. For a realistic error 
of $\epsilon=20$\todo{unit}, the ratio of saved energy with the infinity norm was
close to 20\%, which is substantial.

\begin{table}[]
   \begin{subfigure}{\columnwidth}
   \centering
   \begin{tabular}{l|l|l|l|l|l|l}
   \hline
   \rowcolor{headcolor}
                          & \multicolumn{3}{c|}{Infinity} & \multicolumn{3}{c}{Euclidean} \\\hline
   $\epsilon$              & 100       & 20      & 10       & 100      & 20       & 10    \\
   Max error              & 99.97     & 20.00        & 10.00      & 99.99   & 19.98   & 9.98  \\
   Compr.      ratio      & 88.9\%    & 66.4\%  & 45.5\%   & 87.6\%   & 63.3\%   & 37.2\% \\
   Average mA                & 2.64     & 2.79   & 3.02   & 3.10    & 3.02    & 3.13  \\
   Saved energy              & 23.9\%    & 19.7\%  & 13.0\%  &  10.7\%   & 13.0\%   & 9.68\%\\
   \hline
   \end{tabular}
   \caption{Walking}
   \end{subfigure}
   \begin{subfigure}{\columnwidth}
   \centering
   \begin{tabular}{l|l|l|l|l|l|l}
   \hline
   \rowcolor{headcolor}

                     & \multicolumn{3}{c|}{Infinity} & \multicolumn{3}{c}{Euclidean} \\\hline
   $\epsilon$        & 100       & 20      & 10         & 100      & 20       & 10      \\
   Max error         & 100.00       & 20.00        & 10.00        & 100.00   & 19.99   & 9.96   \\
   Compr.      ratio & 68.6\%    & 25.5\%  & 9.5\%     & 64.4\%   & 19.8\%   & 5.7\%   \\
   Average mA        & 2.88     & 3.22   & 3.38     & 2.95    & 3.32    & 3.39   \\
   Saved energy      & 17.0\%    & 7.18 \%   & 2.50\%   & 14.9\%   & 4.32\%   & 2.22 \%\\
   \hline
   \end{tabular}
   \caption{Running}
   \end{subfigure}
   \caption{Results from Experiment 2}
   \label{table:results-energy}
\end{table}

%~ and the battery usage 
%~ would not change too much, because it contains others fusion method 
%~ which will running when turning on the Neblina. In 50Hz sensor sampling 
%~ rate, the average usage electric current for transmit accelerometer 
%~ data is 2.0mA. It means Neblina could transmit accelerometer data 
%~ constantly in 100/2.0=50 hours. when Neblina does not move, the average 
%~ usage electric current using Infinity norm compression is 1.95mA.

 %~ It 
%~ just extend Neblina work life about 1.28 hours. But in 200Hz sampling 
%~ rate. The electric before and after compression are 3.5 and 2.9 
%~ respectively. The running time of Neblina would be extended about 5.9 
%~ hours.


\section{Conclusion}

We presented an extension of the Lightweight Temporal Compression 
method to dimension $n$ that can be instantiated for any distance 
function. Our extension formulates LTC as an intersection detection 
problem between n-balls. We implemented our extension on Neblina for the 
infinity and Euclidean norms, and we measured the energy reduction 
induced by compression for acceleration streams acquired during
 human activities.

We conclude from our experiments that the proposed extension to LTC is 
well suited to reduce energy consumption in connected objects. The implementation 
behaves better with the infinity norm than with the Euclidean one, due 
to the time complexity of the current algorithm to detect the intersection 
between n-balls for the Euclidean norm.

Our future work focuses on this latter issue. Our current 
implementation of LTC with Euclidean norm relies on Helly's theorem, 
which only provides an algorithm of complexity ${m \choose n+1}$ to 
compress $m$ points in 
dimension $n$. We note that Helly's theorem holds for arbitrary convex 
subsets of $\mathbb{R}^n$, while we are considering a \emph{sequence} 
of \emph{balls} of \emph{decreasing radius}. Based on this 
observation, a stronger result might exist that would lead to a more 
efficient implementation of LTC with the Euclidean norm. Our current 
idea is to search for a point expressed as a function of the ball centers that would 
necessarily belong to the ball intersection when it is not empty; such 
a point, if it exists, necessarily converges to the center of the last 
ball in the sequence as $n$ increases, as the radius of the last ball 
decreases to zero. The resulting algorithm would then compute this 
point and check its inclusion in every ball, which is done in 
$\mathcal{O}(m)$ complexity.

The choice of an appropriate norm should not be underestimated. This problem 
is specific to multi-dimensional streams, since all common norms are 
identical in 1D. Some situations might be better described with the 
Euclidean norm than with the infinity norm, such as the ones involving 
position or movement measures. Using the infinity norm instead of the 
Euclidean would lead to important error differences, proportional to 
$\sqrt{n}$ in dimension $n$. A better algorithm to detect the 
intersection of n-balls in our context would improve the efficiency of LTC
in dimension $n$ for arbitrary norms.

\section*{Acknowledgement}
This work was funded by a Strategic Project Grant of the Natural 
Sciences and Engineering Research Council of Canada. We thank Hosein 
Nourani for providing the data used in Experiment 1.

\bibliographystyle{IEEEtran}
\bibliography{IEEEabrv,biblio.bib}
\end{document}


\end{document}
