\documentclass[10pt, conference, compsocconf]{IEEEtran}

% packages
\usepackage{algorithm}
\usepackage{algorithmicx} % Doc is at http://tug.ctan.org/macros/latex/contrib/algorithmicx/algorithmicx.pdf
\usepackage{algpseudocode}
\usepackage{amsfonts} % for R symbol (the set of real numbers)
\usepackage{color}
\usepackage{colortbl} % for \rowcolor
\usepackage[pdftex]{graphicx}
\usepackage{graphicx}
\usepackage[bookmarks=false]{hyperref}
\hypersetup{colorlinks=true,linkcolor=black,citecolor=black,filecolor=black,urlcolor=blue}
\usepackage{mathtools}
\usepackage{multirow}
\usepackage{stmaryrd} % for llbracket and rrbracket
\usepackage{subcaption}
\usepackage{nicefrac}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{stmaryrd}
\usepackage{amsthm}
\DeclarePairedDelimiter{\ceil}{\lceil}{\rceil}
\DeclarePairedDelimiter{\floor}{\lfloor}{\rfloor}

% new commands
\newcommand{\todo}[1]{\marginpar{\parbox{18mm}{\flushleft\tiny\color{red}\textbf{TODO}:
      #1}}}
\newcommand{\note}[1]{
  \color{blue}\emph{[Note: #1]}
  \color{black}
}
\newcommand{\norm}[1]{\left\lVert#1\right\rVert}
\newtheorem*{theorem}{Theorem}
\definecolor{headcolor}{gray}{0.9}


\begin{document}

\title{A multi-dimensional extension of the Lightweight Temporal Compression method}

\author{Bo Li$^1$, Omid Sarbishei$^2$, Tristan Glatard$^1$\\
  $^1$ Department of Computer Science and Software Engineering, Concordia University, Montreal, QC, Canada \\
  $^2$ Motsai, Saint Bruno, QC, Canada.}
\maketitle

\begin{abstract}
Lightweight Temporal Compression
(LTC) is among the lossy stream
compression methods that provide the highest compression rate for the
lowest resource (CPU, RAM) consumption. As such, it makes it a very
good candidate for the compression of data streams acquired in
energy-constrained systems such as systems on module (SoMs). The type of data acquired on
such systems, however, is often multi-dimensional. For instance 
accelerometers and gyroscopes usually measure variables along 3 
directions, these variables being distinct but related through the 
actual movement of the sensor. In this paper, we investigate the 
extension of LTC to higher dimensions. First, we provide a formulation 
of the algorithm in an arbitrary vectorial space of dimension $n$. 
Then, we implement the algorithm for the infinity and Euclidean norms, 
in spaces of dimension 2D+t and 3D+t. We evaluate our implementation on 
3D acceleration streams acquired by a SoM during human activities. 
Results show that the 3D implementation of LTC can save up to 20\% in 
energy consumption for low-paced activities such as walking. The 
implementation with the infinity norm only requires about 100~B of 
memory.
\end{abstract}

\section{Introduction}

% Compression of stream data
% need references
Compressing data streams is required in many contexts, to reduce 
storage and network requirements of data analytics. In particular, it is 
highly beneficial to energy-constrained devices such as Systems on 
Modules (SoM), as they reduce the amount of energy-consuming radio transmissions.
However, resource-intensive algorithms such as~\cite{x},~\cite{y} 
or~\cite{z} are not well-suited for energy-constrained systems, due to 
the limited memory available on these systems (typically a few KB), and 
the energy consumption associated with CPU usage. On such systems, 
compression algorithms need to find a trade-off between reducing 
network communications and increasing memory and CPU usage. As 
discussed in~\cite{zordan2014performance}, linear compression methods 
provide a very good compromise between these two factors, leading to 
substantial energy reduction.

% lossy vs lossless compression

% LTC summary
% define transmitted, received points, copmression ratio
The Lightweight Temporal Compression method 
(LTC~\cite{schoellhammer2004lightweight}) has been designed 
specifically for energy-constrained systems, initially sensor networks. 
It approximates data points by a piecewise linear function that 
guarantees an upper bound on the reconstruction error. LTC is a very 
lightweight method, with memory requirements in $\mathcal{O}(1)$. 
However, LTC has only been described for 1D streams, which makes it 
limited for multi-dimensional streams such as the ones coming from 
accelerometers, gyroscopes and other movement sensors.

In this paper, we extend LTC to dimension $n$. To do so, we propose an 
algebraic formulation of the algorithm that also yields a 
norm-independent expression of it. We implement our extension on 
Motsai's Neblina module, and we test it on 3D acceleration streams 
acquired during human exercises, namely biceps curling, walking and 
running. Our implementation of LTC is available as free software on GitHub\todo{add link}.

We assume that the stream consists of a sequence of data points 
received at uneven intervals. The compression algorithm 
\emph{transmits} fewer points than it receives. The transmitted points 
might be part of the stream, or computed from stream elements. The 
\emph{compression ratio} is the ratio between the number of received 
points and the number of transmitted points. An application would 
reconstruct the stream from the transmitted points: the 
\emph{reconstruction error} is the maximum absolute difference between 
a data point of the reconstructed stream, and the corresponding data 
point in the original stream.

% Outline
Section~\ref{sec:ltc} provides some background on the LTC algorithm, 
and formalizes the description initially proposed 
in~\cite{schoellhammer2004lightweight}. Section~\ref{sec:extension} 
presents our norm-independent extension to dimension $n$, and 
Section~\ref{sec:implementation} describes our implementation. 
Section~\ref{sec:results} reports on experiments to validate our 
implementation, and evaluate the impact of n-dimensional LTC on SoM 
energy consumption.
\begin{figure}
\includegraphics[width=\columnwidth]{ltc.pdf}
\caption{Illustration of the LTC algorithm and notations used. Red dots indicate transmitted points. Dashed lines
represent the high line and the low line at times when a point is transmitted.}
\label{fig:ltc}
\end{figure}

\section{Lightweight Temporal Compression}
\label{sec:ltc}

LTC approximates the data stream
by a piece-wise linear function of time, ensuring that the error at
every time-point is bounded.

\subsection{Notations}

The algorithm \emph{receives} a stream of \emph{data points} $x_i$
at times $t_i$ ($i \in \mathbb{N}$), and it \emph{transmits} a stream of data points $\xi_i$
at times $\tau_i$ ($i \in \mathbb{N}$). To simplify the notations, we assume that:
\begin{equation*}
\forall k \in \mathbb{N}, \  \exists ! i \in \mathbb{N} \  \tau_k = t_i
\end{equation*}
That is, transmission times coincide with reception times.
We define the \emph{centered received points} as follows:
\begin{equation*}
\forall k \in \mathbb{N}\ , \forall j \in \mathbb{N^*},\ (u^k_j, y^k_j) = (t_{i+j}, x_{i+j}), \quad \mathrm{where\ }i\mathrm{\ is\ s.t.}\ t_i = \tau_k.
\end{equation*}
and:
\begin{equation*}
\forall k \in \mathbb{N},\  (u^k_0, y^k_0) = (\tau_k, \xi_k).
\end{equation*}
This definition is such that $y^k_j$ is the $j^{th}$ data point received
after the $k^{th}$ transmission.
Figure~\ref{fig:ltc} illustrates the notations and algorithm.

The LTC algorithm maintains two lines, the \emph{high line}, and the
\emph{low line} defined by (1) the latest transmitted point and (2) the
\emph{high point} (high line) and the \emph{low point} (low line). When
a point ($t_i$, $x_i$) is received, the high line is updated as
follows: if $x_i+\epsilon$ is below the high line then the high line is
updated to the line defined by the last transmitted point and ($t_i$,
$x_i+\epsilon$); otherwise, the high line is not updated. Likewise, the low line
is updated from $x_i-\epsilon$. Therefore, any line located between the
high line and the low line approximates the data points received since
the last transmitted point with an error bounded by $\epsilon$.

Using these notations, the original LTC algorithm can
be written as in Algorithm~\ref{algo:ltc}. For readability, we assume
that access to data points is blocking, i.e., the program will wait
until the points are available. We also assume that the content of
variable \texttt{tr} is transmitted after each assignment of this
variable. Function \texttt{line}, omitted for brevity, returns the
ordinate at abscissa $x$ (1st argument) of the line defined by the points
in its 2nd and 3rd arguments.

\begin{algorithm}
\begin{algorithmic}[1]
\State tr = $(u^0_0, y^0_0)$ \Comment{last transmitted point}
\State k = 0 ; j = 1
\State (lp, hp) = ($y^1_1 - \epsilon$, $y^1_1 + \epsilon$) \Comment{low and high points}

\While{True} \Comment{Process received points as they come}
    \State j += 1
    \State new\_lp = max($y^k_j-\epsilon$, line($u^k_j$, tr, ($u^k_{j-1}$, lp)))
    \State new\_hp = max($y^k_j+\epsilon$, line($u^k_j$, tr, ($u^k_{j-1}$, hp)))
    \If{new\_lp $<$ new\_hp} \Comment{Keep compressing}
        \State (lp = new\_lp, new\_hp)
        \State \textbf{continue}
    \EndIf
    \State tr = $(u^k_j, (lp+hp)/2)$
    \Comment{Transmit point}
    \State (lp, hp) = ($y^k_j-\epsilon$, $y^k_j+\epsilon$)
    \State k += 1
    \State j = 0
\EndWhile
\end{algorithmic}
\caption{Original LTC algorithm, adapted from~\cite{schoellhammer2004lightweight}.}
\label{algo:ltc}
\end{algorithm}


 %~ is one of the linear estimation
%~ method to compress data. It is The first-order interpolator with two
%~ degrees of freedom (FOI-2DF) which is mentioned in
%~ ~\cite{jalaleddine1990ecg}.  Most of linear estimation method need a
%~ predefined parameter which is the error margin (let's call $\epsilon$)
%~ in order to guarantee the difference between estimation and original
%~ data within a scope.

%~ \todo{rewrite LTC algorithm, and there is a DP version of LTC maybe should also be written}

\section{Extension to dimension $n$}
\label{sec:extension}
In this section we provide a norm-independent formulation of LTC in
dimension $n$. By $n$ we refer to the dimension of the data points
$x_i$. To handle time, LTC actually operates in dimension
$n+1$.

\subsection{Preliminary comments}

We note that the formulation of LTC in~\cite{schoellhammer2004lightweight} relies on
the intersection of \emph{convex cones} in dimension $n+1$. For $n=1$, it
corresponds to the intersection of triangles, which can efficiently be
computed by maintaining boundary lines, as detailed previously. In higher dimension, however, cone intersections are not so
straighforward, due to the fact that the intersection between cones
may not be a cone.

To address this issue, we formulate LTC as an intersection test between
\emph{balls} of dimension $n$, that is, segments for $n=1$, disks for
$n=2$, etc. Balls are defined from the \emph{norm} used in
the vector space of data points. For $n=1$, the choice of the norm does
not really matter, as all p-norms and the infinity norm are identical.
In dimension $n$, however, norm selection will be critical.

\subsection{Algebraic formulation of LTC}

\subsubsection{Definitions}

Let $(u_0^k, y_0^k) \in \mathbb{R}^{n+1}$ be the latest transmitted point. For convenience, all the subsequent points will be
expressed in the orthogonal space with origin $(\tau^k, \xi^k)$. We denote by $(v_j, z_j)_{j \in \llbracket 0, m \rrbracket}$ such points:
\begin{equation*}
\forall j \leq m,\  (v_j, z_j) = (u_j^k - \tau^k, y_j^k - \xi^k)
\end{equation*}
We define $\mathcal{B}_j$ as the ball of $\mathbb{R}^n$ of center $\frac{1}{j}z_j$ and of radius
$\frac{\epsilon}{j}$:
\begin{equation*}
\mathcal{B}_j = \left\{ z \in \mathbb{R}^n,\  \norm{z-\frac{v_1}{v_j}z_j} \leq \frac{v_1}{v_j}\epsilon \right\}
\end{equation*}

\subsection{LTC property}

We formulate the \emph{LTC property} as follows:
\begin{equation*}
\exists z \in \mathbb{R}^n, \ \forall j \in \llbracket 1, m \rrbracket, \norm{\frac{v_j}{v_1}z-z_j} \leq \epsilon.
\end{equation*}
The LTC algorithm ensures that the LTC property is
verified between each transmission. Indeed, all the data points
$z$ such that $(v_1, z)$ is between the high line and the low line
verify the property. Line 9 in Algorithm~\ref{algo:ltc} guarantees that
such a point exists.
The LTC property can be written as follows:
\begin{equation*}
\exists z \in \mathbb{R}^n, \ \forall j \in \llbracket 1, m \rrbracket, \norm{z-\frac{v_1}{v_j}z_j} \leq \frac{v_1}{v_j}\epsilon
\end{equation*}
that is:
\begin{equation}
\bigcap_{j=1}^m \mathcal{B}_j \neq \O
\label{eq:ltc-property}
\end{equation}
Note that $(\mathcal{B}_j)_{j \in \llbracket 1, m \rrbracket}$ is a sequence
of balls of strictly decreasing radius, since $v_j > v_1$.
\todo{Add a figure to illustrate this}

\subsection{Algorithm}

The LTC algorithm generalized to dimension $n$ test, for each data point,
that the LTC property in Equation~\ref{eq:ltc-property} is verified. It is written in
Algorithm~\ref{algo:general-ltc}.
\begin{algorithm}
\begin{algorithmic}[1]
\State tr = ($\tau, \xi$) = ($u^0_0, y^0_0$)
\State k = 0 ; j = 0
\While{True}
    \State j += 1
    \State ($v_j, z_j$) = ($u_j^k - \tau, y_j^k - \xi$)
    \If{$\bigcap_{l=1}^j{\mathcal{B}_l} \neq \O$}
        \State \textbf{continue}
    \EndIf
    \State Pick $z$ in $\bigcap_{l=1}^{j-1}{\mathcal{B}_j}$
    \State tr = ($\tau$, $\xi$) = ($u^k_{j-1}, z$)
    \State k += 1
    \State j = 0
\EndWhile
\end{algorithmic}
\caption{Generalized LTC}
\label{algo:general-ltc}
\end{algorithm}

\todo{Check algorithms (reimplement)}

\subsection{Ball intersections}

Although Algorithm~\ref{algo:general-ltc} looks simple, one should not
overlook the fact that there is no good general algorithm to test
whether a set of balls intersect. The best general algorithm we could find
so far relies on Helly's theorem which is formulated as follows~\cite{helly1923mengen}:
\begin{theorem}
Let $\left\{ X_i \right\}_{i \in \llbracket 1, m \rrbracket}$ be a collection of convex subsets of $\mathbb{R}^n$. If the intersection of every $n+1$
subsets is non-empty, then the whole collection has an non-empty intersection.
\end{theorem}
This theorem leads to an algorithm of complexity ${m \choose n+1}$ which is
not usable in resource-constrained environment.

The only feasible algorithm that we found is norm-specific. It
maintains a representation of the intersection
$\bigcap_{j=1}^{m}{\mathcal{B}_m}$ which is updated at every iteration.
The intersection tests can then be done in constant time, however,
updating the representation of the intersection may be more costly
depending on the norm used. For the infinity norm, the representation
is a rectangular cuboid which is straightforward to update by
intersection with an n-ball, since the n-ball is a rectangular cuboid too.
For the Euclidean norm, the representation is a volume with no specific property,
which is more costly to maintain.

\subsection{Effect of the norm}

As mentioned before, norm selection in $\mathbb{R}^n$ has a critical
impact on the compression error and ratio. To appreciate this effect,
let us consider the comparison between the infinity norm and the
Euclidean norm in dimension 2. By comparing the unit disk to the unit
square, we obtain that the compression ratio of a random stream would
be $\frac{4}{\pi}$ times larger with the infinity norm than with the 
Euclidean norm. In 3D, this ratio would be $\frac{6}{\pi}$. Conversely, 
a compression error bounded by $\epsilon$ with the infinity norm 
corresponds to a compression error of $\frac{\epsilon}{\sqrt{n}}$ with 
the Euclidean norm. Unsurprisingly, the
infinity norm is more 'tolerant' than the Euclidean norm.

\todo{Perhaps add a figure}

It should also be noted that using the infinity norm boils down to the
use of the 1D LTC algorithm independently in each dimension: a data
point is transmitted as soon as the linear approximation doesn't hold
in any of the coordinates. \todo{This is unclear}


\section{Implementation}
\label{sec:implementation}
Our code is available at ... (make a clean release!)

%~ As reference above, the norm selection is importance. In this section,
%~ we show the implementation of infinity norm and Euclidean norm.

%~ \subsection{Multidimensional LTC with Manhattan distance}
%~ In section 2, we can know that the repersentation of intersection
%~ is a rectangular cuboid. So only one bounding box with n-dimensional
%~ is needed to represent the intersection. Assume a bounding box
%~ $\mathcal{BOX}_{m-1} = \bigcap_{j=1}^{m-1}{\mathcal{B}_{m-1}}$ which
%~ has Upper bound $U_i$ and lower Bound $L_i$ for each dimension
%~ $i$. So the fomular (1) in $6_{th}$ line of Algorithm 2 would
%~ be changed to $\mathcal{BOX}_{m-1} \bigcap \mathcal{B}_{m} \neq \O $. The Bounding box
%~ would be updated when this formular return true.
%~ Otherwise return false.
%~ \begin{enumerate}
    %~ \item Initialization: Let origin $(u^k_0, y^k_0) = (\tau_k, \xi_k)$.
    %~ \item For each data $(u_m^k, y_m^k)$ with diminsion $n$,
            %~ defines $(v_m, z_m) = (u_m^k - \tau^k, y_m^k - \xi^k)$ where $z_m =
            %~ \{ x_{mi} \mid 1\leqslant{i}\leqslant{n}\}$.
    %~ \item The $\mathcal{B}_{m}$ should be a ractangular cube with center
            %~ $\frac{v_1}{v_m}z_m$ and weight $\frac{v_1}{v_m}\epsilon$.
    %~ \item For each dimension $i$, box upper bound $U_i < \frac{v_1}{v_m}x_{mi}+
            %~ \frac{v_1}{v_m}\epsilon$ OR lower bound $L_i > \frac{v_1}{v_m}x_{mi}-\frac{v_1}{v_m}\epsilon$, return False.
    %~ \item Otherwise, the intersection do not be empty. Updating
            %~ the Bounding box $\mathcal{BOX}_m = \mathcal{BOX}_{m-1} \bigcap \mathcal{B}_{m}$. For
            %~ each dimension $i$ in $\mathcal{BOX}_m$, $U_i = min(U_i, \frac{v_1}{v_m}x_{mi}+
            %~ \frac{v_1}{v_m}\epsilon)$, and $L_i = max(L_i , \frac{v_1}{v_m}x_{mi}-\frac{v_1}{v_m}\epsilon)$. Return True.
%~ \end{enumerate}


%~ According the Implement, the infinity norm boils down to apply 1D LTC algorithm in each dimension respectively. Since, the intersection would be rectangle in 2D and cube in 3D, the corresponding max errors are $\frac{\epsilon}{\sqrt{2}}$ and $\frac{\epsilon}{\sqrt{3}}$. As we mentioned in Section 2, the infinity norm is more 'tolerant' than Euclidean norm. The max error between original data and reconstructed data certainly not greater than $\frac{\epsilon}{\sqrt{n}}$ where $n$ is dimension of data.

%~ \todo{maybe need a graphic}

%~ \subsection{Multidimensional LTC with Euclidean distance}
%~ In Euclidea norm, the representation of intersection is
%~ volume. There is no specific model to record repersentation
%~ with updating, so a list to save all balls $\mathcal{B}_j, 1\leqslant{j}\leqslant{m}$
%~ is needed. In order to test weather a set of balls intersect,
%~ the Helly's theorem could work, but the complexity of
%~ this method is too high to work with embeded, low-power
%~ systems which has limited CPU and RAM resource. So we
%~ implement Euclidean norm by using a method which based
%~ on plane sweep and binary search. Assume the list of balls $S_{m-1}$ contains balls from $\mathcal{B}_1$ to $\mathcal{B}_{m-1}$, and an temporary data point $output$ used for updated origin $(\tau^k, \xi^k)$. From the formula (1) in Section 3, the $(\mathcal{B}_j)_{j \in \llbracket 1, {m-1} \rrbracket}$ is a sequence of decreasing radius, so the larger balls that includes new coming ball $\mathcal{B}_{m-1}$ could be reomved from list $S_{m-1}$ so that reducing processing complexity of method. Therefore the list $S_{m-1}$ may not includes $m-1$ balls. Assume each $\Hat{\mathcal{B}} \in S$ which has corresponding center $\Hat{z}$ and radius $\Hat{\epsilon}$.
%~ Let origin $(u^k_0, y^k_0) = (\tau_k, \xi_k)$, and for each data $(u_m^k, y_m^k)$ with diminsion $n$, defines $(v_m, z_m) = (u_m^k - \tau^k, y_m^k - \xi^k)$ where $z_m = \{ x_{mi} \mid 1\leqslant{i}\leqslant{n}\}$. The $\mathcal{B}_{m}$ should be one n-ball with center $\frac{v_1}{v_j}z_m$ and radius $\frac{v_1}{v_j}\epsilon$. The method is written in Algorithm~\ref{algo:isThereIntersection}.

%~ \begin{algorithm}
    %~ \begin{flushleft}
        %~ \textbf{Input:} $S$, $\mathcal{B}_m$, $output$ \\
        %~ \textbf{Output:} $true|false$\\
    %~ \end{flushleft}
    %~ \begin{algorithmic}[1]
        %~ \State d = n \Comment{dimension index}
        %~ \State $max_d = \frac{v_1}{v_m}(x_d + \epsilon)$
        %~ \State $min_d = \frac{v_1}{v_m}(x_d - \epsilon)$
        %~ \ForAll{$\Hat{\mathcal{B}}$ \textbf{in} $S$}
            %~ \If{$\Hat{\mathcal{B}} \bigcap \mathcal{B}_m = $ \O }
                %~ \State \textbf{return} false
            %~ \ElsIf{$\mathcal{B}_m \bigcap \Hat{\mathcal{B}}  = \mathcal{B}_m $}
                %~ \State remove $\Hat{\mathcal{B}}$ from $S$
                %~ \State \textbf{continue}\\
            %~ $max_d = \min(max_d, \Hat{x_d} + \Hat{\epsilon}$)\\
            %~ $min_d = \max(min_d, \Hat{x_d} - \Hat{\epsilon}$)
            %~ \EndIf
        %~ \EndFor
        %~ \State $\textbf{add } \mathcal{B}_m \textbf{ into } S$
        %~ \If{$max_d < min_d$}
            %~ \State \textbf{return} false
        %~ \EndIf
        %~ \State temp\_output = output \Comment{used for next operation}
        %~ \If{Recursive$(min_d, max_d, n, p\_cp)$}
            %~ \State $output = temp\_output$
            %~ \State \textbf{return} true
        %~ \Else
            %~ \State \textbf{return} false
        %~ \EndIf
    %~ \end{algorithmic}
    %~ \caption{isThereIntersection}
    %~ \label{algo:isThereIntersection}
%~ \end{algorithm}

%~ In the method, selecting dimension(n as default in our method) to calculate max point value and min point value of all n-balls in this dimension. If the max value bigger than min value, then there is no intersection. After that. In Recursive function Algorithm~\ref{algo:recursive}, using binary search select
%~ median $mid_d$ of $max_d$ and $min_d$ in order to find bound range of dimension d-1 ($max_{d-1}$, $min_{d-1}$) over the plane $x_d = max_d$. If bound range of dimension $d-1$ is not empty, continue calling function \textbf{Recursive}($min_{d-1}$, $min_{d-1}$, $d-1$, $temp\_output$). Recursively calling this function, until getting result.
%~ \todo{add recursive pseudo-code}

%~ In 2-dimension Euclidean norm, the complexity of this algorithm is $O(n)+O(n\log\epsilon) = O(n\log\epsilon)$. In 3-dimension, It needs $O(\log^2\epsilon)$ to determine $mid\_3, mid\_2$, and $O(n)$ to traverse all balls in list and calculate bounding range of dimension d=1. So 3-dimensional method need $O(n \log^2\epsilon)$ totally. If we extend this idea for n-dimension, the complexiy of this method is $O(n \log^{n}\epsilon)$ where n is the number of dimension.

%~ \begin{algorithm}
    %~ \begin{flushleft}
        %~ \textbf{function} Recursive$(left, right, j, p)$  -- $j$ means $j_th$ dimension
    %~ \end{flushleft}
    %~ \begin{algorithmic}[1]
        %~ \While{$left \leqslant right$}
            %~ \State $mid \gets (left + right)/2$
            %~ \State $max \gets +\infty$
            %~ \State $min \gets -\infty$
            %~ \For{$i=1$ \textbf{to} $tmp\_list.length$}
                %~ \State $P_1$ and $P_2$ are intersection points of $tmp\_list[i]$ and line $d_{j}=mid$, $(P1.d_{j-1}\geqslant P2.d_{j-1})$
                %~ \If{$P_1.d_{j-1} < max$}
                    %~ \State $max \gets p_1.d_{j-1}$
                    %~ \State $max\_index \gets i$
                %~ \EndIf
                %~ \If{$P_2.d_{j-1} > min$}
                    %~ \State $min \gets p_2.d_{j-1}$
                    %~ \State $min\_index \gets i$
                %~ \EndIf
            %~ \EndFor
            %~ \If{$max >= min$}
                %~ \State $p.d_j \gets mid$
                %~ \If{Recursive$(min, max, j-1, p)$}
                    %~ \State \textbf{return} true
                %~ \EndIf
            %~ \EndIf
            %~ \State Assume $P_d$ is the intersection between common chord of two objects $tmp\_list[max\_index]$, $tmp\_list[min\_index]$ and their line of centers.
            %~ \If{$P_d.d_j < mid$}
                %~ \State $right \gets mid-1$
            %~ \ElsIf{$P_d.d_j > mid$}
                %~ \State $left \gets mid+1$
            %~ \EndIf

        %~ \EndWhile
        %~ \Return false
    %~ \end{algorithmic}
%~ \end{algorithm}

\section{Experiments and Results}
\label{sec:results}

We conducted two experiments using Motsai's Neblina 
module\footnote{\url{ https://motsai.com/products/neblina}}, a system 
with a Nordic Semiconductor nRF52832 micro-controller, 64~KB of RAM, 
and Bluetooth Low Energy connectivity. The Neblina has a 3D 
accelerometer, a 3D gyroscope, a 3D magnetometer, and environmental sensors 
for humidity, temperature and pressure. It can function as an Inertial 
Motion Unit, a Vertical Reference Unit, or an Attitude and Heading 
Reference System. The Neblina has a battery of 100mAh; its average 
consumption is 2.52~mA without radio transmission and 3.47~mA with 
radio transmission, leading to an autonomy of 39.7~h without 
transmission and 28.8~h with transmission. \todo{Omid, could you review
this paragraph?}

\subsection{Experiment 1: validation tests}

In this experiment we validated the behavior of our LTC extensions on a 
PC processing data acquired with the Neblina. We collected two 3D 
accelerometer time series, a short one and a longer one, acquired 
on two different subjects performing biceps curl, with a 50Hz sampling rate (see 
Figure~\ref{fig:datasets-1}). It should be noted that the longer dataset also has 
a higher amplitude, most likely due to differences between subjects.

\begin{figure*}
\begin{subfigure}{2\columnwidth}
\includegraphics[width=0.33\columnwidth]{figures/5-time-bicep-curl-plot-x.pdf}
\includegraphics[width=0.33\columnwidth]{figures/5-time-bicep-curl-plot-y.pdf}
\includegraphics[width=0.33\columnwidth]{figures/5-time-bicep-curl-plot-z.pdf}
\caption{Short}
\end{subfigure}
\begin{subfigure}{2\columnwidth}
\includegraphics[width=0.33\columnwidth]{figures/Mohammad-bicep-curl-plot-x.pdf}
\includegraphics[width=0.33\columnwidth]{figures/Mohammad-bicep-curl-plot-y.pdf}
\includegraphics[width=0.33\columnwidth]{figures/Mohammad-bicep-curl-plot-z.pdf}
\caption{Long} 
\end{subfigure}
\caption{Time series used in Experiment 1}
\label{fig:datasets-1}
\end{figure*}

We compressed the time series with various values of $\epsilon$, using our 
2D (x and y) and 3D (x, y and z) implementations of LTC. On the Neblina,
the resolution of the accelerometer is about 20~mg (196~mm/$s^2$).
 We used a 
laptop computer with 16~GB of memory, and Intel i5-3210M CPU @ 2.50GHz 
Ã— 4, and Linux Fedora 27. We measured memory consumption using 
Valgrind's massif 
tool~\cite{nethercote2006building}\footnote{\url{http://valgrind.org}}, 
and processing time using \texttt{gettimeofday()} from the GNU C 
Library. 

Results are reported in Table~\ref{table:results-validation}. 
As expected, the compression ratio increases with $\epsilon$, and the 
maximum measured error remains lower than $\epsilon$ in all cases. The 
maximum is reached most of the time on these time series.

\paragraph{Inifinity vs Euclidean norms}
Compared to the infinity norm, The average ratio between the compression ratios obtained
with the infinity and Euclidean norm is 1.08 for 2D data, and 1.20 
for 3D data. These ratios are lower than the theoretical values of 
$\frac{4}{\pi}$ in 2D and $\frac{6}{\pi}$ in 3D, which are obtained for 
random-uniform signals. Unsurprisingly, the infinity norm surpasses the 
Euclidean norm in terms of resource consumption. Memory-wise, the 
infinity norm requires a constant amount of 80~B, used to store the 
intersection of n-balls. This amount is realistic on modules such as
the Neblina. The Euclidean norm, however, uses up to 4.9~KB of memory 
on the Long time series in 3D with $\epsilon=100$. More importantly, 
the amount of required memory increases for more complex time series 
(Long vs Short), and it also increases with larger 
values of $\epsilon$. Similar observations are made for the processing 
time, with values ranging from 0.2~ms for the simplest time series and 
smallest $\epsilon$, to 26.4~ms for the most complex time series and 
largest $\epsilon$. A better algorithm needs to be found to make the 
Euclidean norm usable for systems on modules.

\paragraph{2D vs 3D}
For a given $\epsilon$ of 100 and a given time series, the compression 
ratios are always higher in 2D than in 3D. It makes sense since the 
\todo{x} is higher in 3D than it is in 2D. Resource consumption is 
higher in 3D than in 2D: for the infinity norm, 3D consumes 1.4 times 
more memory than 2D (1.6 times on average for Euclidean norm), and 
processing time is 1.3 longer on average (1.18 on average for Euclidean 
norm).

\begin{table}
    \begin{subfigure}{\columnwidth}
    \centering
    \begin{tabular}{l|l|l|l|l}
    \hline
    \rowcolor{headcolor}
                           & \multicolumn{2}{c|}{Infinity} & \multicolumn{2}{c}{Euclidean}\\ \hline
    $\epsilon$                & 100          & 100/$\sqrt{2}$  & 100         & 100/$\sqrt{2}$ \\
    Max error              & 99.88       & 70.68           & 99.63       & 70.47          \\ 
    Compression ratio      & 29.03\%      & 20.88\%         & 25.12\%     & 19.57\%        \\ 
    Peak memory            & 80~B          & 80~B             & 432~B        & 240~B           \\ 
    Processing time        & 0.103~ms      & 0.082~ms         & 0.220~ms     & 0.200~ms        \\ \hline
    \end{tabular}
    \caption{Short (2D)}
    \end{subfigure}\\
    \begin{subfigure}{\columnwidth}
    \centering
    \begin{tabular}{l|l|l|l|l}
    \hline
    \rowcolor{headcolor}
                   & \multicolumn{2}{c|}{Infinity} & \multicolumn{2}{c}{Euclidean} \\ \hline
    $\epsilon$             & 100        & 100/$\sqrt{2}$    & 100        & 100/$\sqrt{2}$    \\ 
    Max error              & 99.99     & 70.71             & 99.99      & 70.71             \\ 
    Compression ratio      & 60.17\%    & 51.59\%           & 57.75\%    & 48.86\%           \\ 
    Peak memory       & 80~B        & 80~B               & 2.1~KB      & 1.3~KB             \\ 
    Processing time      & 5.70~ms     & 4.85~ms            & 20.04~ms    & 19.28~ms           \\ \hline
    \end{tabular}
    \caption{Long (2D)}
    \end{subfigure}\\    
    \begin{subfigure}{\columnwidth}
    \centering
    \begin{tabular}{l|l|l|l|l}
    \hline
    \rowcolor{headcolor}
                            & \multicolumn{2}{c|}{Infinity} & \multicolumn{2}{c}{Euclidean} \\ \hline
    $\epsilon$             & 100          & 100/$\sqrt{3}$  & 100         & 100/$\sqrt{3}$   \\ 
    Max error              & 99.88       & 57.41           & 99.94       & 56.89            \\ 
    Compression ratio      & 23.00\%      & 12.89\%         & 18.43\%     & 9.95\%           \\ 
    Peak memory            & 112~B         & 112~B            & 384~B        & 256~B             \\ 
    Processing time        & 0.119~ms      & 0.094~ms         & 0.230~ms     & 0.200~ms          \\ \hline
    \end{tabular}
    \caption{Short (3D)}
    \end{subfigure}\\
    \begin{subfigure}{\columnwidth}
    \centering
    \begin{tabular}{l|l|l|l|l}
    \hline
    \rowcolor{headcolor}
                              & \multicolumn{2}{c|}{Infinity} & \multicolumn{2}{c}{Euclidean} \\ \hline
    $\epsilon$                & 100        & 100/$\sqrt{3}$    & 100        & 100/$\sqrt{3}$    \\
    Max error                 & 99.96     & 57.73             & 99.99      & 57.73             \\
    Compression ratio         & 51.13\%    & 37.17\%           & 46.05\%    & 32.63\%           \\
    Peak memory & 112~B        & 112~B              & 4.9~KB      & 3.0~KB             \\
    Processing time           & 8.50~ms     & 7.07~ms            & 26.41~ms    & 21.26~ms           \\ \hline
    \end{tabular}
    \caption{Long (3D)}
    \end{subfigure}
    \caption{Results from Experiment 1}
    \label{table:results-validation}
\end{table}

\subsection{Experiment 2: impact on energy consumption}

We measured the energy consumption of the Neblina module 
for different values of $\epsilon$ and norms, to quantify the 
added-value of LTC for this system. 

We acquired two 3D accelerometer time series at 200Hz corresponding to 
common human activities: walking and running (see Figure~\ref{fig:datasets-2}). In both cases, the 
subject was wearing the Neblina module on their right wrist.
We collected 1,000 data points for each activity, corresponding to 
5 seconds of activity.
\begin{figure*}
\begin{subfigure}{2\columnwidth}
\includegraphics[width=0.33\columnwidth]{./figures/walking-1000-x.pdf}
\includegraphics[width=0.33\columnwidth]{./figures/walking-1000-y.pdf}
\includegraphics[width=0.33\columnwidth]{./figures/walking-1000-z.pdf}
\caption{Walking}
\end{subfigure}
\begin{subfigure}{2\columnwidth}
\includegraphics[width=0.33\columnwidth]{./figures/running-1000-x.pdf}
\includegraphics[width=0.33\columnwidth]{./figures/running-1000-y.pdf}
\includegraphics[width=0.33\columnwidth]{./figures/running-1000-z.pdf}
\caption{Running}
\end{subfigure}
\caption{Time series used in Experiment 2}
\label{fig:datasets-2}
\end{figure*}

We measured energy consumption associated with the transmission of 
these time series, for different norms and $\epsilon$ values. To do so, 
we 'replayed' the time series by loading them as a byte array in the 
Neblina, and we measured current intensity every 500~ms.

Results are reported in Table~\ref{table:results-energy}\todo{errors don't make sense}.
For a given $\epsilon$ and norm, the compression ratio was larger for 
walking than for running, since the running signal had a higher 
entropy. The ratio of saved energy is relative to the reference current of 
3.47~mA measured when the Neblina transmits data without compression.
In all cases, activating compression saved energy. The reduction in energy
consumption behaved as the compression ratio: it increased with $\epsilon$,
was higher for the infinity norm than for the Euclidean, and it was 
higher for the walking activity than for running. For a realistic error 
of $\epsilon=20$\todo{unit}, the ratio of saved energy with the infinity norm was
close to 20\%, which is very substantial.

\begin{table}[]
    \begin{subfigure}{\columnwidth}
    \centering
    \begin{tabular}{l|l|l|l|l|l|l}
    \hline
    \rowcolor{headcolor}
                           & \multicolumn{3}{c|}{Infinity} & \multicolumn{3}{c}{Euclidean} \\\hline
    $\epsilon$              & 100       & 20      & 10      & 100       & 20      & 10   \\
    Max error              & 158.793   & 31.828  & 16.763  & 153.141   & 31.763  & 15.898  \\
    Compr.      ratio      & 88.9\%    & 66.4\%  & 45.5\%  & 68.6\%    & 25.5\%  & 9.5\%   \\
    Average mA    & 2.642     & 2.787   & 3.020   & 2.879     & 3.221   & 3.383   \\
    Saved energy  & 23.9\%    & 19.7\%  & 13.0\%  & 17.0\%    & 7.18 \% & 2.50\%\\
    \hline
    \end{tabular}
    \caption{Walking}
    \end{subfigure}
    \begin{subfigure}{\columnwidth}
    \centering
    \begin{tabular}{l|l|l|l|l|l|l}
    \hline
    \rowcolor{headcolor}
                      & \multicolumn{3}{c|}{Infinity} & \multicolumn{3}{c}{Euclidean} \\\hline
    $\epsilon$         & 100      & 20       & 10      & 100      & 20       & 10      \\
    Max error         & 99.995   & 19.984   & 9.979   & 99.995   & 19.989   & 9.959   \\
    Compr.      ratio & 87.6\%   & 63.3\%   & 37.2\%  & 64.4\%   & 19.8\%   & 5.7\%   \\
    Average mA      & 3.098    & 3.020    & 3.134   & 2.952    & 3.320    & 3.393   \\
    Saved energy   &  10.7\%   & 13.0\%   & 9.68\%  & 14.9\%   & 4.32\%   & 2.22 \%\\
    \hline
    \end{tabular}
    \caption{Running}
    \end{subfigure}
    \caption{Results from Experiment 2}
    \label{table:results-energy}
\end{table}

%~ and the battery usage 
%~ would not change too much, because it contains others fusion method 
%~ which will running when turning on the Neblina. In 50Hz sensor sampling 
%~ rate, the average usage electric current for transmit accelerometer 
%~ data is 2.0mA. It means Neblina could transmit accelerometer data 
%~ constantly in 100/2.0=50 hours. when Neblina does not move, the average 
%~ usage electric current using Infinity norm compression is 1.95mA.

 %~ It 
%~ just extend Neblina work life about 1.28 hours. But in 200Hz sampling 
%~ rate. The electric before and after compression are 3.5 and 2.9 
%~ respectively. The running time of Neblina would be extended about 5.9 
%~ hours.





\section{Conclusion}

Euclidean norm would benefit from a better algorithm but it still
provides interesting energy savings.

\section*{Acknowledgement}
This work was funded by the Canadian 

\bibliographystyle{IEEEtran}
\bibliography{IEEEabrv,biblio.bib}
\end{document}


\end{document}
