\documentclass[10pt, conference, compsocconf]{IEEEtran}

% packages
\usepackage{algorithm}
\usepackage{algorithmicx} % Doc is at http://tug.ctan.org/macros/latex/contrib/algorithmicx/algorithmicx.pdf
\usepackage{algpseudocode}
\usepackage{amsfonts} % for R symbol (the set of real numbers)
\usepackage{color}
\usepackage{colortbl} % for \rowcolor
\usepackage[pdftex]{graphicx}
\usepackage{graphicx}
\usepackage[bookmarks=false]{hyperref}
\hypersetup{colorlinks=true,linkcolor=black,citecolor=black,filecolor=black,urlcolor=blue}
\usepackage{mathtools}
\usepackage{multirow}
\usepackage{stmaryrd} % for llbracket and rrbracket
\usepackage{subcaption}
\usepackage{nicefrac}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{stmaryrd}
\usepackage{amsthm}
\DeclarePairedDelimiter{\ceil}{\lceil}{\rceil}
\DeclarePairedDelimiter{\floor}{\lfloor}{\rfloor}

\algblock{Input}{EndInput}
\algnotext{EndInput}
\algblock{Output}{EndOutput}
\algnotext{EndOutput}
\newcommand{\Desc}[2]{\State \makebox[2em][l]{#1}#2}

% new commands
\newcommand{\todo}[1]{\marginpar{\parbox{18mm}{\flushleft\tiny\color{red}\textbf{TODO}:
      #1}}}
\newcommand{\note}[1]{
  \color{blue}\emph{[Note: #1]}
  \color{black}
}
\newcommand{\norm}[1]{\left\lVert#1\right\rVert}
\newtheorem*{theorem}{Theorem}
\definecolor{headcolor}{gray}{0.9}


\begin{document}

\title{A multi-dimensional extension of the Lightweight Temporal Compression method}

\author{Bo Li$^1$, Omid Sarbishei$^2$, Tristan Glatard$^1$\\
  $^1$ Department of Computer Science and Software Engineering, Concordia University, Montreal, QC, Canada \\
  $^2$  Research and Development Department, Motsai Research, Saint Bruno, QC, Canada}
\maketitle

\begin{abstract}
Lightweight Temporal Compression
(LTC) is among the lossy stream
compression methods that provide the highest compression rate for the 
lowest CPU and memory consumption. As such, it is very well suited to 
compress data streams in energy-constrained systems such as connected 
objects. The current formulation of LTC, however, is one-dimensional 
while data acquired in connected objects is often multi-dimensional: for instance, 
accelerometers and gyroscopes usually measure variables along 
3 directions related by the actual movement of the sensor. In this 
paper, we investigate the extension of LTC to 
higher dimensions. First, we provide a formulation 
of the algorithm in an arbitrary vectorial space of dimension $n$. 
Then, we implement the algorithm for the infinity and Euclidean norms, 
in spaces of dimension 2D+t and 3D+t. We evaluate our implementation on 
3D acceleration streams acquired during human activities. 
Results show that the 3D implementation of LTC can save up to 20\% in 
energy consumption for low-paced activities, with a memory usage of about 100~B.
\end{abstract}

\section{Introduction}

With the recent technological advances in Internet of Things (IoT) 
applications, more than one billion connected objects are expected to 
be launched worldwide by 2025\footnote{\url{ 
https://www.statista.com/statistics/471264/iot-number-of-connected-devices-worldwide}} 
Power consumption is amongst the biggest challenges targeting connected 
objects, particularly in the industrial domains, where several sensing 
systems are commonly launched in the field to run for days or even 
weeks without being recharged. Typically, such devices use sensors to 
capture properties such as temperature or motion, and stream them to a 
host system over a radio transmission protocol such as Bluetooth 
Low-Energy (BLE). System designers aim to reduce the rate of data 
transmission as much as possible, as radio transmission is a 
power-hungry operation.

 Compression is a key technique to reduce the rate of radio 
 transmission.  While in several applications lossless compression 
 methods are more desirable than lossy compression techniques, in the 
 context of IoT and sensor data streams, the measured sensor data 
 intrinsically involves noise and measurement errors, which can 
 be treated as a configurable 
tolerance for a lossy compression algorithm. 

Resource-intensive lossy compression algorithms such as the ones based on 
polynomial interpolation, discrete cosine and Fourier transforms, or 
auto-regression methods~\cite{lu2010optimized} are not well-suited for 
energy-constrained systems, due to the limited memory available on 
these systems (typically a few KB), and the energy consumption 
associated with CPU usage. On such systems, compression algorithms need 
to find a trade-off between reducing network communications and 
increasing memory and CPU usage. As 
discussed in~\cite{zordan2014performance}, linear compression methods 
provide a very good compromise between these two factors, leading to 
substantial energy reduction.

% lossy vs lossless compression

% LTC summary
% define transmitted, received points, copmression ratio
The Lightweight Temporal Compression method 
(LTC~\cite{schoellhammer2004lightweight}) has been designed 
specifically for energy-constrained systems, initially sensor networks. 
It approximates data points by a piecewise linear function that 
guarantees an upper bound on the reconstruction error, and a reduced 
memory footprint in $\mathcal{O}(1)$. However, LTC has only been 
described for 1D streams, while streams acquired by connected objects, such as 
acceleration or gyroscopic data, are often multi-dimensional. 

In this paper, we extend LTC to dimension $n$. To do so, we propose an 
algebraic formulation of the algorithm that also yields a 
norm-independent expression of it. We implement our extension on 
Motsai's Neblina module, and we test it on 3D acceleration streams 
acquired during human exercises, namely biceps curling, walking and 
running. Our implementation of LTC is available as free software.

We assume that the stream consists of a sequence of data points 
received at uneven intervals. The compression algorithm 
\emph{transmits} fewer points than it receives. The transmitted points 
might be included in the stream, or computed from stream points. The 
\emph{compression ratio} is the ratio between the number of received 
points and the number of transmitted points. An application 
reconstructs the stream from the transmitted points: the 
\emph{reconstruction error} is the maximum absolute difference between 
a point of the reconstructed stream, and the corresponding 
point in the original stream.

\begin{figure}
\includegraphics[width=\columnwidth]{ltc.pdf}
\caption{Illustration of the LTC algorithm. Blue 
dots are received points, red dots are transmitted points. Dashed lines 
represent the high and low lines when a point is 
transmitted.\vspace*{-0.3cm}}
\label{fig:ltc}
\end{figure}

\todo{show $\xi$ in the Figure}
\todo{Fix Omid's comment in the Figure}

% Outline
Section~\ref{sec:ltc} provides some background on the LTC algorithm, 
and formalizes the description initially proposed 
in~\cite{schoellhammer2004lightweight}. Section~\ref{sec:extension} 
presents our norm-independent extension to dimension $n$, and 
Section~\ref{sec:implementation} describes our implementation. 
Section~\ref{sec:results} reports on experiments to validate our 
implementation, and evaluates the impact of n-dimensional LTC on 
energy consumption of connected objects.

\newpage

\section{Lightweight Temporal Compression}
\label{sec:ltc}

LTC approximates the data stream
by a piece-wise linear function of time, with an error bounded by parameter $\epsilon$.

\subsection{Notations}

The algorithm receives a stream of data points $x_i$
at times $t_i$ ($i \in \mathbb{N}$), and it transmits a stream of data points $\xi_i$
at times $\tau_i$ ($i \in \mathbb{N}$). To simplify the notations, we assume that:
\begin{equation*}
\forall k \in \mathbb{N}, \  \exists ! i \in \mathbb{N} \  \tau_k = t_i
\end{equation*}
That is, transmission times coincide with reception times.
We define the \emph{shifted received points} as follows:
\begin{equation*}
\forall k \in \mathbb{N}\ , \forall j \in \mathbb{N^*},\ (u^k_j, y^k_j) = (t_{i+j}, x_{i+j}), 
\end{equation*}
where $i$ is such that $t_i = \tau_k$ and:
\begin{equation*}
\forall k \in \mathbb{N},\  (u^k_0, y^k_0) = (\tau_k, \xi_k).
\end{equation*}
This definition is such that $y^k_j$ is the $j^{th}$ data point received
after the $k^{th}$ transmission and $u^k_j$ is the corresponding timestamp.
Figure~\ref{fig:ltc} illustrates the notations and algorithm.

The LTC algorithm maintains two lines, the \emph{high line}, and the
\emph{low line} defined by (1) the latest transmitted point and (2) the
\emph{high point} (high line) and the \emph{low point} (low line). When
a point ($t_i$, $x_i$) is received, the high line is updated as
follows: if $x_i+\epsilon$ is below the high line then the high line is
updated to the line defined by the last transmitted point and ($t_i$,
$x_i+\epsilon$); otherwise, the high line is not updated. Likewise, the low line
is updated from $x_i-\epsilon$. Therefore, any line located between the
high line and the low line approximates the data points received since
the last transmitted point with an error bounded by $\epsilon$.

\begin{algorithm}
\begin{algorithmic}[1]
\Input
   \Desc{$(u^k_j, y^k_j)$}{$\quad \quad $Received data stream}
   \Desc{$\epsilon$}{$\quad \quad$Error bound}
\EndInput
\Output
   \Desc{tr}{Transmitted points}
\EndOutput
\State tr = $(u^0_0, y^0_0)$ \Comment{last transmitted point}
\State k = 0 ; j = 1
\State (lp, hp) = ($y^1_1 - \epsilon$, $y^1_1 + \epsilon$) \Comment{low and high points}

\While{True} \Comment{Process received points as they come}
    \State j += 1
    \State new\_lp = max($y^k_j-\epsilon$, line($u^k_j$, tr, ($u^k_{j-1}$, lp)))
    \State new\_hp = max($y^k_j+\epsilon$, line($u^k_j$, tr, ($u^k_{j-1}$, hp)))
    \If{new\_lp $<$ new\_hp} \Comment{Keep compressing}
        \State (lp, hp) = (new\_lp, new\_hp)
        \State \textbf{continue}
    \EndIf
    \State tr = $(u^k_j, (lp+hp)/2)$
    \Comment{Transmit point}
    \State (lp, hp) = ($y^k_j-\epsilon$, $y^k_j+\epsilon$)
    \State k += 1
    \State j = 1
\EndWhile
\end{algorithmic}
\caption{Original LTC algorithm, adapted from~\cite{schoellhammer2004lightweight}.}
\label{algo:ltc}
\end{algorithm}

Using these notations, the original LTC algorithm can
be written as in Algorithm~\ref{algo:ltc}. For readability, we assume
that access to data points is blocking, i.e., the program will wait
until the points are available. We also assume that the content of
variable \texttt{tr} is transmitted after each assignment of this
variable. Function \texttt{line}, omitted for brevity, returns the
ordinate at abscissa $x$ (1st argument) of the line defined by the points
in its 2nd and 3rd arguments.

\section{Extension to dimension $n$}
\label{sec:extension}
In this section we provide a norm-independent formulation of LTC in
dimension $n$. By $n$ we refer to the dimension of the data points
$x_i$. To handle time, LTC actually operates in dimension
$n+1$.

\subsection{Preliminary comments}

We note that the formulation of LTC in~\cite{schoellhammer2004lightweight} relies on
the intersection of \emph{convex cones} in dimension $n+1$. For $n=1$, it
corresponds to the intersection of triangles, which can efficiently be
computed by maintaining boundary lines, as detailed previously. In higher dimension, however, cone intersections are not so
straighforward, due to the fact that the intersection between cones
may not be a cone.

To address this issue, we formulate LTC as an intersection test between
\emph{balls} of dimension $n$, that is, segments for $n=1$, disks for
$n=2$, etc. Balls are defined from the \emph{norm} used in
the vector space of data points. For $n=1$, the choice of the norm does
not really matter, as all p-norms and the infinity norm are identical.
In dimension $n$, however, norm selection will be critical.

\subsection{Algebraic formulation of LTC}

\subsubsection{Definitions}

Let $(u_0^k, y_0^k) \in \mathbb{R}^{n+1}$ be the latest transmitted point. For convenience, all the subsequent points will be
expressed in the orthogonal space with origin $(\tau_k, \xi_k)$. We denote by $(v_j, z_j)_{j \in \llbracket 0, m \rrbracket}$ such points:
\begin{equation*}
\forall j \leq m,\  (v_j, z_j) = (u_j^k - \tau_k, y_j^k - \xi_k)
\end{equation*}
Let $\mathcal{B}_j$ be the ball of $\mathbb{R}^n$ of center $\frac{v_1}{v_j}z_j$ and radius
$\frac{v_1}{v_j}\epsilon$:
\begin{equation*}
\mathcal{B}_j = \left\{ z \in \mathbb{R}^n,\  \norm{z-\frac{v_1}{v_j}z_j} \leq \frac{v_1}{v_j}\epsilon \right\}
\end{equation*}

\subsubsection{LTC property}

We define the \emph{LTC property} as follows:
\begin{equation*}
\exists z \in \mathbb{R}^n, \ \forall j \in \llbracket 1, m \rrbracket, \norm{\frac{v_j}{v_1}z-z_j} \leq \epsilon.
\end{equation*}
The original LTC algorithm ensures that the LTC property is
verified between each transmission. Indeed, all the data points
$z$ such that $(v_1, z)$ is between the high line and the low line
verify the property. Line 9 in Algorithm~\ref{algo:ltc} guarantees that
such a point exists.

The LTC property can be re-written as follows:
\begin{equation*}
\exists z \in \mathbb{R}^n, \ \forall j \in \llbracket 1, m \rrbracket, \norm{z-\frac{v_1}{v_j}z_j} \leq \frac{v_1}{v_j}\epsilon
\end{equation*}
that is:
\begin{equation}
\bigcap_{j=1}^m \mathcal{B}_j \neq \O
\label{eq:ltc-property}
\end{equation}
Note that $(\mathcal{B}_j)_{j \in \llbracket 1, m \rrbracket}$ is a sequence
of balls of strictly decreasing radius, since $v_j > v_1$.

\subsection{Algorithm}

The LTC algorithm generalized to dimension $n$ tests that the LTC 
property in Equation~\ref{eq:ltc-property} is verified after each reception of a data 
point. It is written in Algorithm~\ref{algo:general-ltc}.
\begin{algorithm}
\begin{algorithmic}[1]
\Input
   \Desc{$(u^k_j, y^k_j)$}{$\quad \quad $Received data stream}
   \Desc{$\epsilon$}{$\quad \quad$Error bound}
\EndInput
\Output
   \Desc{tr}{Transmitted points}
\EndOutput

\State tr = ($\tau, \xi$) = ($u^0_0, y^0_0$)
\State k = 0 ; j = 0
\While{True}
    \State j += 1
    \State ($v_j, z_j$) = ($u_j^k - \tau, y_j^k - \xi$)
    \If{$\bigcap_{l=1}^j{\mathcal{B}_l} \neq \O$}
        \State \textbf{continue}
    \EndIf
    \State Pick $z$ in $\bigcap_{l=1}^{j-1}{\mathcal{B}_j}$
    \State tr = ($\tau$, $\xi$) = ($u^k_{j-1}, z$)
    \State k += 1
    \State j = 0
\EndWhile
\end{algorithmic}
\caption{Generalized LTC}
\label{algo:general-ltc}
\end{algorithm}

\todo{Check algorithms (reimplement)}

\subsection{Ball intersections}

Although Algorithm~\ref{algo:general-ltc} looks simple, one should not
overlook the fact that there is no good general algorithm to test
whether a set of balls intersect. The best general algorithm we could find
so far relies on Helly's theorem which is formulated as follows~\cite{helly1923mengen}:
\begin{theorem}
Let $\left\{ X_i \right\}_{i \in \llbracket 1, m \rrbracket}$ be a collection of convex subsets of $\mathbb{R}^n$. If the intersection of every $n+1$
subsets is non-empty, then the whole collection has an non-empty intersection.
\end{theorem}
\noindent This theorem leads to an algorithm of complexity ${m \choose n+1}$ which is
not usable in resource-constrained environment.

The only feasible algorithm that we found is norm-specific. It
maintains a representation of the intersection
$\bigcap_{j=1}^{m}{\mathcal{B}_j}$ which is updated at every iteration.
The intersection tests can then be done in constant time. However,
updating the representation of the intersection may be costly
depending on the norm used. For the infinity norm, the representation
is a rectangular cuboid which is straightforward to update by
intersection with an n-ball.
For the Euclidean norm, the representation is a volume with no particular property,
which is more costly to maintain.

\subsection{Effect of the norm}

As mentioned before, norm selection in $\mathbb{R}^n$ has a critical
impact on the compression error and ratio. To appreciate this effect,
let us compare the infinity norm and the
Euclidean norm in dimension 2. By comparing the unit disk to a
square of side 2, we obtain that the compression ratio of a random stream would
be $\frac{4}{\pi}$ times larger with the infinity norm than with the 
Euclidean norm. In 3D, this ratio would be $\frac{6}{\pi}$. Conversely, 
a compression error bounded by $\epsilon$ with the infinity norm 
corresponds to a compression error of $\frac{\epsilon}{\sqrt{n}}$ with 
the Euclidean norm. Unsurprisingly, the
infinity norm is more `tolerant' than the Euclidean norm.

It should also be noted that using the infinity norm in $\mathbb{R}^n$ 
boils down to the use of the 1D LTC algorithm independently in each 
dimension, since a data point will be transmitted as soon as the linear 
approximation doesn't hold in any of the dimensions.

\section{Implementation}
\label{sec:implementation}

To implement LTC with the infinity norm, we maintain a cuboid 
representation of $\cap_{l=1}^j{\mathcal{B}_l}$ throughout the 
iterations of the \texttt{while} loop in 
Algorithm~\ref{algo:general-ltc}. The implementation works with
constant memory and requires limited CPU time.

With the Euclidean norm, the intersection test is more complex. We keep 
in memory a growing set $S$ of balls and the bounding box $B$ of their 
intersection. Then, when a new point arrives, we consider the 
associated ball $\mathcal{B}_j$ and our intersection test works as in 
Algorithm~\ref{algo:euclidean}. \texttt{box} is a function that returns 
the bounding box of an n-ball. \texttt{find\_bisection(S, B)} searches 
for a point in all the elements in S, using plane sweep and bisection 
initialized by the bounds of B. Our code is available at 
\url{https://github.com/big-data-lab-team/stream-summarization} under 
MIT license.

\begin{algorithm}
\begin{algorithmic}[1]
\Input
   \Desc{$S$}{Set of intersecting balls}
   \Desc{$B$}{Bounding box of the intersection of balls in $S$}
   \Desc{$\mathcal{B}_j$}{New ball to check}
\EndInput
\Output
   \Desc{$S$}{Updated set of intersecting balls}
   \Desc{$B$}{Updated bounding box}
   \Desc{$T$}{True if all the balls in S and $\mathcal{B}_j$ intersect}
\EndOutput

\If{$\exists\ \mathcal{B}_i \in S$ s.t. $\mathcal{B}_j \cap \mathcal{B}_i = \O$}
\State \Return ($S$, False)
\EndIf
\If{$\exists\ \mathcal{B}_i \in S$ s.t. $\mathcal{B}_j \subset \mathcal{B}_i$}
\State Remove $\mathcal{B}_i$ from $B$. Add $\mathcal{B}_j$ to $B$.
\State \Return ($S$, $B$, True)
\EndIf
\If{$\mathcal{B}_j \cap B = \O$}
\State \Return ($S$, $B$, False)
\EndIf
\State $B$ = box($\mathcal{B}_j$) $\ \bigcap\ $ B 
\State $S$ = $S \ \bigcup \  \{\mathcal{B}_j\}$
\State $x$ = find\_bisection($S$, $B$)
\If{$x$ == Null}
\State \Return ($S$, $B$, False)
\Else
\State \Return ($S$, $B$, True)
\EndIf
\end{algorithmic}
\caption{Intersection test for Euclidean n-balls.}
\label{algo:euclidean}
\end{algorithm}

\section{Experiments and Results}
\label{sec:results}

We conducted two experiments using Motsai's Neblina 
module\footnote{\url{ https://motsai.com/products/neblina}}, a system 
with a Nordic Semiconductor nRF52832 micro-controller, 64~KB of RAM, 
and Bluetooth Low Energy connectivity. Neblina has a 3D 
accelerometer, a 3D gyroscope, a 3D magnetometer, and environmental 
sensors for humidity, temperature and pressure. The platform is 
equipped with sensor fusion algorithms for 3D orientation tracking and 
a machine learning engine for complex motion analysis and motion 
pattern recognition~\cite{sarbishei2016accuracy}. Neblina has a 
battery of 100mAh; its average consumption is 2.52~mA when using 
accelerometer and gyroscope sensors at 200~Hz but without radio 
transmission, and 3.47~mA with radio transmission, leading to an 
autonomy of 39.7~h without transmission and 28.8~h with transmission. 

\subsection{Experiment 1: validation}

%\todo{(Put a picture of Neblina in a box worn 
%on the wrist)}

We validated the behavior of our LTC extension on a 
PC using data acquired with Neblina. We collected two 3D 
accelerometer time series, a short one and a longer one, acquired 
on two different subjects performing biceps curl, with a 50Hz sampling rate (see 
Figure~\ref{fig:datasets-1}). In both cases, the 
subject was wearing Neblina on their right wrist. It should be noted that the longer dataset also has 
a higher amplitude, most likely due to differences between subjects.

\begin{figure*}
\begin{subfigure}{2\columnwidth}
\includegraphics[width=0.3\columnwidth]{figures/5-time-bicep-curl-plot-x.pdf}
\includegraphics[width=0.3\columnwidth]{figures/5-time-bicep-curl-plot-y.pdf}
\includegraphics[width=0.3\columnwidth]{figures/5-time-bicep-curl-plot-z.pdf}
\caption{Short biceps curl$^a$}
\end{subfigure}
\begin{subfigure}{2\columnwidth}
\includegraphics[width=0.3\columnwidth]{figures/Mohammad-bicep-curl-plot-x.pdf}
\includegraphics[width=0.3\columnwidth]{figures/Mohammad-bicep-curl-plot-y.pdf}
\includegraphics[width=0.3\columnwidth]{figures/Mohammad-bicep-curl-plot-z.pdf}
\caption{Long biceps curl}
\end{subfigure}
\caption{Time series used in Experiment 1\\
{\footnotesize $^a$ Average of az data is -7.81g. It was shifted to 0 so that the graphs can all use the same y scale.}}
\label{fig:datasets-1}
\end{figure*}

We compressed the time series with various values of $\epsilon$, using 
our 2D (x and y) and 3D (x, y and z) implementations of LTC. On 
Neblina, the raw uncalibrated accelerometer data corresponds to errors 
around 20~mg (1~g is 9.8~m/s$^2$). We used a 
laptop computer with 16~GB of RAM, an Intel i5-3210M CPU @ 2.50GHz 
× 4, and Linux Fedora 27. We measured memory consumption using 
Valgrind's massif 
tool~\cite{nethercote2006building}\footnote{\url{http://valgrind.org}}, 
and processing time using \texttt{gettimeofday()} from the GNU C 
Library. 

Results are reported in Table~\ref{table:results-validation}. 
As expected, the compression ratio increases with $\epsilon$, and the 
maximum measured error remains lower than $\epsilon$ in all cases. The 
maximum is reached most of the time on these time series.

\paragraph{Infinity vs Euclidean norms}
Compared to the infinity norm, the average ratio between the compression ratios obtained
with the infinity and Euclidean norm is 1.08 for 2D data, and 1.20
for 3D data. These ratios are lower than the theoretical values of
$\frac{4}{\pi}$ in 2D and $\frac{6}{\pi}$ in 3D, which are obtained for
random-uniform signals. Unsurprisingly, the infinity norm surpasses the
Euclidean norm in terms of resource consumption. Memory-wise, the
infinity norm requires a constant amount of 80~B, used to store the
intersection of n-balls. The Euclidean norm, however, uses up to 4.9~KB of memory
for the Long time series in 3D with $\epsilon$=48.8~mg. More importantly,
the amount of required memory increases for more complex time series
(Long vs Short), and it also increases with larger
values of $\epsilon$. Similar observations are made for the processing
time, with values ranging from 0.2~ms for the simplest time series and
smallest $\epsilon$, to 26.4~ms for the most complex time series and
largest~$\epsilon$.
%~ Figure~\ref{fig:memory} shows the memory consumption 
%~ of the 3D Euclidean implementation for $\epsilon$=48.8~mg: peaks appear
%~ at the end of long compressed ranges where the signal was $\epsilon$-closed
%~ to the linear approximation.

\paragraph{2D vs 3D}
For a given $\epsilon$, the compression 
ratios are always higher in 2D than in 3D. It makes sense since the
probability for the signal to deviate from a straight line
approximation is higher in 3D than it is in 2D. Besides, resource
consumption is higher in 3D than in 2D: for the infinity norm, 3D
consumes 1.4 times more memory than 2D (1.6 times on average for
Euclidean norm), and processing time is 1.3 longer (1.18 on
average for Euclidean norm).

\begin{table}
    \begin{subfigure}{\columnwidth}
    \centering
    \begin{tabular}{l|l|l|l|l}
    \hline
    \rowcolor{headcolor}
                           & \multicolumn{2}{c|}{Infinity} & \multicolumn{2}{c}{Euclidean}\\ \hline
    $\epsilon$  (mg)          & 48.8         & 34.5       & 48.8       & 34.5 \\
    Max error   (mg)          & 48.8         & 34.5       & 48.6       & 34.4 \\
    Compression ratio (\%)    & 29.03        & 20.88      & 25.12      & 19.57\\
    Peak memory   (B)         & 80           & 80         & 432        & 240  \\
    Processing time (ms)      & 0.103        & 0.082      & 0.220s     & 0.200\\ \hline
    \end{tabular}
    \caption{Short biceps curl (2D)}
    \end{subfigure}\\
    \begin{subfigure}{\columnwidth}
    \centering
    \begin{tabular}{l|l|l|l|l}
    \hline
    \rowcolor{headcolor}
                   & \multicolumn{2}{c|}{Infinity} & \multicolumn{2}{c}{Euclidean} \\ \hline
    $\epsilon$ (mg)            & 48.8        & 34.5       & 48.8        & 34.5    \\
    Max error  (mg)            & 48.8        & 34.5       & 48.8        & 34.5             \\
    Compression ratio (\%)     & 60.17       & 51.59      & 57.75       & 48.86           \\
    Peak memory  (B)           & 80          & 80         & 2100        & 2500             \\
    Processing time (ms)       & 5.70        & 4.85       & 20.04       & 19.28           \\ \hline
    \end{tabular}
    \caption{Long biceps curl (2D)}
    \end{subfigure}\\
    \begin{subfigure}{\columnwidth}
    \centering
    \begin{tabular}{l|l|l|l|l}
    \hline
    \rowcolor{headcolor}
                           & \multicolumn{2}{c|}{Infinity} & \multicolumn{2}{c}{Euclidean} \\ \hline
    $\epsilon$ (mg)        & 48.8          & 28.2          & 48.8   & 28.2   \\
    Max error  (mg)        & 48.8          & 28.0          & 48.8   & 27.8   \\
    Compression ratio (\%) & 23.00         & 12.89         & 18.43  & 9.95   \\
    Peak memory (B)        & 112           & 112           & 384    & 256    \\
    Processing time (ms)   & 0.119         & 0.094         & 0.230  & 0.200  \\ \hline
    \end{tabular}
    \caption{Short biceps curl (3D)}
    \end{subfigure}\\
    \begin{subfigure}{\columnwidth}
    \centering
    \begin{tabular}{l|l|l|l|l}
    \hline
    \rowcolor{headcolor}
                              & \multicolumn{2}{c|}{Infinity} & \multicolumn{2}{c}{Euclidean} \\ \hline
    $\epsilon$ (mg)                & 48.8        & 28.2       & 48.8     & 28.2    \\
    Max error  (mg)                & 48.8        & 28.2       & 48.8     & 28.2    \\
    Compression ratio (\%)         & 51.13       & 37.17      & 46.05    & 32.63   \\
    Peak memory (B)                & 112         & 112        & 4900     & 3000    \\
    Processing time (ms)           & 8.50        & 7.07       & 26.41    & 21.26   \\ \hline
    \end{tabular}
    \caption{Long biceps curl (3D)}
    \end{subfigure}
    \caption{Results from Experiment 1}
    \label{table:results-validation}
\end{table}

\begin{figure*}
\begin{subfigure}{2\columnwidth}
\includegraphics[width=0.3\columnwidth]{./figures/walking-1000-x.pdf}
\includegraphics[width=0.3\columnwidth]{./figures/walking-1000-y.pdf}
\includegraphics[width=0.3\columnwidth]{./figures/walking-1000-z.pdf}
\caption{Walking}
\end{subfigure}
\begin{subfigure}{2\columnwidth}
\includegraphics[width=0.3\columnwidth]{./figures/running-1000-x.pdf}
\includegraphics[width=0.3\columnwidth]{./figures/running-1000-y.pdf}
\includegraphics[width=0.3\columnwidth]{./figures/running-1000-z.pdf}
\caption{Running}
\end{subfigure}
\caption{Time series used in Experiment 2 \vspace*{-0.3cm}}
\label{fig:datasets-2}
\end{figure*}

%~ \begin{figure}
%~ \includegraphics[width=\columnwidth]{./figures/memory-mohammad.pdf}
%~ \caption{Memory usage of 3D Euclidean implementation, long biceps curl time series, $\epsilon$=48.8~mg.}
%~ \label{fig:memory}
%~ \end{figure}

\subsection{Experiment 2: impact on energy consumption}

%~ We measured the energy consumption of Neblina
%~ for different values of $\epsilon$ and norms, to quantify the
%~ added-value of LTC for this system.

We acquired two 3D accelerometer time series at 200~Hz for two 
activities: walking and running (see Figure~\ref{fig:datasets-2}). In 
both cases, the subject was wearing Neblina on their right wrist. We 
collected 1,000 data points for each activity, corresponding to 5 
seconds of activity.

We measured energy consumption associated with the transmission of
these time series by
`replaying' the time series after loading them as a byte array in
Neblina. We measured the current every 500~ms.

 Results are reported in Table~\ref{table:results-energy}. For a given
 $\epsilon$ and norm, the compression ratio is larger for walking than
 for running. The ratio of saved energy is relative to the reference
 current of 3.47~mA measured when Neblina transmits data without
 compression. In all cases, activating compression saves energy. The
 reduction in energy consumption behaves as the compression ratio: it
 increases with $\epsilon$,
it is higher for the infinity norm than for the Euclidean, and it is
higher for the walking activity than for running. For a realistic error
of $\epsilon$=9.8~mg, the ratio of saved energy with the infinity norm was
close to 20\%, which is substantial.

\begin{table}[]
   \begin{subfigure}{\columnwidth}
   \centering
   \begin{tabular}{l|l|l|l|l|l|l}
   \hline
   \rowcolor{headcolor}
                          & \multicolumn{3}{c|}{Infinity} & \multicolumn{3}{c}{Euclidean} \\\hline
   $\epsilon$ (mg)             & 48.8       & 9.8      & 4.9       & 48.8      & 9.8       & 4.9    \\
   Max error (mg)              & 48.8     & 9.8        & 4.9      & 48.8   & 9.8   & 4.9  \\
   Compr.      ratio (\%)      & 88.9    & 66.4  & 45.5   & 87.6   & 63.3   & 37.2 \\
   Average (mA)                & 2.64     & 2.79   & 3.02   & 3.10    & 3.02    & 3.13  \\
   Saved energy (\%)              & 23.9    & 19.7  & 13.0  &  10.7   & 13.0   & 9.7\\
   \hline
   \end{tabular}
   \caption{Walking}
   \end{subfigure}
   \begin{subfigure}{\columnwidth}
   \centering
   \begin{tabular}{l|l|l|l|l|l|l}
   \hline
   \rowcolor{headcolor}

                     & \multicolumn{3}{c|}{Infinity} & \multicolumn{3}{c}{Euclidean} \\\hline
   $\epsilon$ (mg)        & 48.8       & 9.8      & 4.9       & 48.8      & 9.8    & 4.9    \\
   Max error (mg)         & 48.8       & 9.8      & 4.9       & 48.8      & 9.8    & 4.9   \\
   Compr.      ratio (\%) & 68.6       & 25.5     & 9.5       & 64.4      & 19.8   & 5.7   \\
   Average (mA)        & 2.88     & 3.22   & 3.38     & 2.95    & 3.32    & 3.39   \\
   Saved energy (\%)      & 17.0    & 7.2    & 2.5   & 14.9   & 4.3   & 2.2\\
   \hline
   \end{tabular}
   \caption{Running}
   \end{subfigure}
   \caption{Results from Experiment 2}
   \label{table:results-energy}
\end{table}

%~ and the battery usage 
%~ would not change too much, because it contains others fusion method 
%~ which will running when turning on the Neblina. In 50Hz sensor sampling 
%~ rate, the average usage electric current for transmit accelerometer 
%~ data is 2.0mA. It means Neblina could transmit accelerometer data 
%~ constantly in 100/2.0=50 hours. when Neblina does not move, the average 
%~ usage electric current using Infinity norm compression is 1.95mA.

 %~ It 
%~ just extend Neblina work life about 1.28 hours. But in 200Hz sampling 
%~ rate. The electric before and after compression are 3.5 and 2.9 
%~ respectively. The running time of Neblina would be extended about 5.9 
%~ hours.


\section{Conclusion}

We presented an extension of the Lightweight Temporal Compression 
method to dimension $n$ that can be instantiated for any distance 
function. Our extension formulates LTC as an intersection detection 
problem between n-balls. We implemented our extension on Neblina for the 
infinity and Euclidean norms, and we measured the energy reduction 
induced by compression for acceleration streams acquired during
 human activities.

We conclude from our experiments that the proposed extension to LTC is 
well suited to reduce energy consumption in connected objects. The implementation 
behaves better with the infinity norm than with the Euclidean one, due 
to the time complexity of the current algorithm to detect the intersection 
between n-balls for the Euclidean norm.

Our future work focuses on this latter issue. Our current 
implementation of LTC with Euclidean norm relies on Helly's theorem, 
which only provides an algorithm of complexity ${m \choose n+1}$ to 
compress $m$ points in 
dimension $n$. We note that Helly's theorem holds for arbitrary convex 
subsets of $\mathbb{R}^n$, while we are considering a \emph{sequence} 
of \emph{balls} of \emph{decreasing radius}. Based on this 
observation, a stronger result might exist that would lead to a more 
efficient implementation of LTC with the Euclidean norm. Our current 
idea is to search for a point expressed as a function of the ball centers that would 
necessarily belong to the ball intersection when it is not empty; such 
a point, if it exists, necessarily converges to the center of the last 
ball in the sequence as $n$ increases, as the radius of the last ball 
decreases to zero. The resulting algorithm would then compute this 
point and check its inclusion in every ball, which is done in 
$\mathcal{O}(m)$ complexity.

The choice of an appropriate norm should not be underestimated. This problem 
is specific to multi-dimensional streams, since all common norms are 
identical in 1D. Some situations might be better described with the 
Euclidean norm than with the infinity norm, such as the ones involving 
position or movement measures. Using the infinity norm instead of the 
Euclidean would lead to important error differences, proportional to 
$\sqrt{n}$ in dimension $n$. A better algorithm to detect the 
intersection of n-balls in our context would improve the efficiency of 
LTC in dimension $n$ for arbitrary norms. Investigating other norms, in 
particular the 1-norm, would be relevant too.

\section*{Acknowledgement}
This work was funded by the Natural 
Sciences and Engineering Research Council of Canada. We thank Hosein 
Nourani for providing data for Experiment 1.

\bibliographystyle{IEEEtran}
\bibliography{IEEEabrv,biblio.bib}
\end{document}


\end{document}
